version: '3.8'

services:
  # ==================== Zookeeper ====================
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SYNC_LIMIT: 2
      ZOOKEEPER_INIT_LIMIT: 5
      ZOOKEEPER_MAX_CLIENT_CNXNS: 60
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_logs:/var/lib/zookeeper/log
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "bash", "-c", "echo 'ruok' | nc localhost 2181"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - legal-network

  # ==================== Kafka Brokers ====================
  kafka1:
    image: confluentinc/cp-kafka:7.4.0
    hostname: kafka1
    container_name: kafka1
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "29092:29092"
      - "19092:19092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9092,OUTSIDE://0.0.0.0:29092
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka1:9092,OUTSIDE://localhost:29092
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_JMX_PORT: 19092
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=kafka1 -Dcom.sun.management.jmxremote.rmi.port=19092
      # 성능 최적화 설정
      KAFKA_NUM_NETWORK_THREADS: 8
      KAFKA_NUM_IO_THREADS: 8
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
      KAFKA_LOG_RETENTION_HOURS: 168  # 7일
      KAFKA_LOG_SEGMENT_BYTES: 1073741824  # 1GB
      KAFKA_LOG_CLEANUP_POLICY: delete
    volumes:
      - kafka1_data:/var/lib/kafka/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 30s
      timeout: 10s
      retries: 10
    networks:
      - legal-network

  kafka2:
    image: confluentinc/cp-kafka:7.4.0
    hostname: kafka2
    container_name: kafka2
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9093:9093"
      - "29093:29093"
      - "19093:19093"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9093,OUTSIDE://0.0.0.0:29093
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka2:9093,OUTSIDE://localhost:29093
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_JMX_PORT: 19093
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=kafka2 -Dcom.sun.management.jmxremote.rmi.port=19093
      # 성능 최적화 설정 (kafka1과 동일)
      KAFKA_NUM_NETWORK_THREADS: 8
      KAFKA_NUM_IO_THREADS: 8
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_CLEANUP_POLICY: delete
    volumes:
      - kafka2_data:/var/lib/kafka/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9093", "--list"]
      interval: 30s
      timeout: 10s
      retries: 10
    networks:
      - legal-network

  kafka3:
    image: confluentinc/cp-kafka:7.4.0
    hostname: kafka3
    container_name: kafka3
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9094:9094"
      - "29094:29094"
      - "19094:19094"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INSIDE:PLAINTEXT,OUTSIDE:PLAINTEXT
      KAFKA_LISTENERS: INSIDE://0.0.0.0:9094,OUTSIDE://0.0.0.0:29094
      KAFKA_ADVERTISED_LISTENERS: INSIDE://kafka3:9094,OUTSIDE://localhost:29094
      KAFKA_INTER_BROKER_LISTENER_NAME: INSIDE
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'false'
      KAFKA_JMX_PORT: 19094
      KAFKA_JMX_OPTS: -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Djava.rmi.server.hostname=kafka3 -Dcom.sun.management.jmxremote.rmi.port=19094
      # 성능 최적화 설정 (kafka1과 동일)
      KAFKA_NUM_NETWORK_THREADS: 8
      KAFKA_NUM_IO_THREADS: 8
      KAFKA_SOCKET_SEND_BUFFER_BYTES: 102400
      KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: 102400
      KAFKA_SOCKET_REQUEST_MAX_BYTES: 104857600
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_LOG_CLEANUP_POLICY: delete
    volumes:
      - kafka3_data:/var/lib/kafka/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "kafka-topics", "--bootstrap-server", "localhost:9094", "--list"]
      interval: 30s
      timeout: 10s
      retries: 10
    networks:
      - legal-network

  # ==================== Kafka UI ====================
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka1
      - kafka2
      - kafka3
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: legal-kafka-cluster
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka1:9092,kafka2:9093,kafka3:9094
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
      KAFKA_CLUSTERS_0_READONLY: false
      KAFKA_CLUSTERS_0_JMXPORT: 19092
      KAFKA_CLUSTERS_0_SCHEMAREGISTRY: http://schema-registry:8081
      DYNAMIC_CONFIG_ENABLED: 'true'
    restart: unless-stopped
    networks:
      - legal-network

  # ==================== Schema Registry ====================
  schema-registry:
    image: confluentinc/cp-schema-registry:7.4.0
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      - kafka1
      - kafka2
      - kafka3
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka1:9092,kafka2:9093,kafka3:9094
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC: _schemas
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC_REPLICATION_FACTOR: 3
      SCHEMA_REGISTRY_DEBUG: 'false'
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8081/subjects"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - legal-network

  # ==================== MySQL Blue-Green ====================
  mysql-blue:
    image: mysql:8.0
    container_name: mysql-blue
    hostname: mysql-blue
    ports:
      - "3306:3306"
    environment:
      MYSQL_ROOT_PASSWORD: legal_root_2024!
      MYSQL_DATABASE: legal_db
      MYSQL_USER: legal_user
      MYSQL_PASSWORD: legal_pass_2024!
      # 성능 최적화
      MYSQL_INNODB_BUFFER_POOL_SIZE: 1G
      MYSQL_INNODB_LOG_FILE_SIZE: 256M
      MYSQL_INNODB_FLUSH_LOG_AT_TRX_COMMIT: 2
      MYSQL_INNODB_FLUSH_METHOD: O_DIRECT
      MYSQL_MAX_CONNECTIONS: 200
    command: >
      --character-set-server=utf8mb4
      --collation-server=utf8mb4_unicode_ci
      --default-authentication-plugin=mysql_native_password
      --innodb-buffer-pool-size=1G
      --innodb-log-file-size=256M
      --innodb-flush-log-at-trx-commit=2
      --innodb-flush-method=O_DIRECT
      --max-connections=200
      --slow-query-log=1
      --slow-query-log-file=/var/log/mysql/slow.log
      --long-query-time=2
      --log-error=/var/log/mysql/error.log
      --general-log=0
    volumes:
      - mysql_blue_data:/var/lib/mysql
      - mysql_blue_logs:/var/log/mysql
      - ./src/database/schema.sql:/docker-entrypoint-initdb.d/01-schema.sql:ro
      - ./docker/mysql/init-blue.sql:/docker-entrypoint-initdb.d/02-init-blue.sql:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "legal_user", "-plegal_pass_2024!"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - legal-network

  mysql-green:
    image: mysql:8.0
    container_name: mysql-green
    hostname: mysql-green
    ports:
      - "3307:3306"
    environment:
      MYSQL_ROOT_PASSWORD: legal_root_2024!
      MYSQL_DATABASE: legal_db
      MYSQL_USER: legal_user
      MYSQL_PASSWORD: legal_pass_2024!
      # 성능 최적화 (Blue와 동일)
      MYSQL_INNODB_BUFFER_POOL_SIZE: 1G
      MYSQL_INNODB_LOG_FILE_SIZE: 256M
      MYSQL_INNODB_FLUSH_LOG_AT_TRX_COMMIT: 2
      MYSQL_INNODB_FLUSH_METHOD: O_DIRECT
      MYSQL_MAX_CONNECTIONS: 200
    command: >
      --character-set-server=utf8mb4
      --collation-server=utf8mb4_unicode_ci
      --default-authentication-plugin=mysql_native_password
      --innodb-buffer-pool-size=1G
      --innodb-log-file-size=256M
      --innodb-flush-log-at-trx-commit=2
      --innodb-flush-method=O_DIRECT
      --max-connections=200
      --slow-query-log=1
      --slow-query-log-file=/var/log/mysql/slow.log
      --long-query-time=2
      --log-error=/var/log/mysql/error.log
      --general-log=0
    volumes:
      - mysql_green_data:/var/lib/mysql
      - mysql_green_logs:/var/log/mysql
      - ./src/database/schema.sql:/docker-entrypoint-initdb.d/01-schema.sql:ro
      - ./docker/mysql/init-green.sql:/docker-entrypoint-initdb.d/02-init-green.sql:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "legal_user", "-plegal_pass_2024!"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - legal-network

  # ==================== Redis (캐싱 및 세션) ====================
  redis:
    image: redis:7.0-alpine
    container_name: redis
    hostname: redis
    ports:
      - "6379:6379"
    environment:
      REDIS_PASSWORD: legal_redis_2024!
    command: >
      redis-server
      --requirepass legal_redis_2024!
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --appendonly yes
      --appendfsync everysec
    volumes:
      - redis_data:/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "legal_redis_2024!", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - legal-network

  # ==================== Apache Airflow ====================
  postgres:
    image: postgres:13
    container_name: postgres
    hostname: postgres
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow_pass_2024!
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - legal-network

  airflow-webserver:
    image: custom-airflow:2.8.4
    container_name: airflow-webserver
    hostname: airflow-webserver
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow_pass_2024!@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: redis://:legal_redis_2024!@redis:6379/1
      AIRFLOW__CELERY__BROKER_URL: redis://:legal_redis_2024!@redis:6379/2
      AIRFLOW__CORE__FERNET_KEY: 'Zv8eB7X0eH1K5k8F7yP9mN3wQ6tR2sA1dG4hJ8kL0oM='
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
      AIRFLOW_CONN_MYSQL_BLUE: 'mysql://legal_user:legal_pass_2024!@mysql-blue:3306/legal_db'
      AIRFLOW_CONN_MYSQL_GREEN: 'mysql://legal_user:legal_pass_2024!@mysql-green:3307/legal_db'
      AIRFLOW_CONN_KAFKA: 'kafka://kafka1:9092,kafka2:9093,kafka3:9094'
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: airflow
      _AIRFLOW_WWW_USER_PASSWORD: airflow_admin_2024!
    volumes:
      - ./src:/opt/airflow/dags
      - ./src/airflow/logs:/opt/airflow/logs
      - ./src/airflow/plugins:/opt/airflow/plugins
      - ./src/airflow/config:/opt/airflow/config
      - airflow_logs:/opt/airflow/logs
    ports:
      - "8090:8080"  # Changed from 8080 to avoid conflict with Kafka UI
    command: webserver
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - legal-network

  airflow-scheduler:
    image: custom-airflow:2.8.4
    container_name: airflow-scheduler
    hostname: airflow-scheduler
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow_pass_2024!@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: redis://:legal_redis_2024!@redis:6379/1
      AIRFLOW__CELERY__BROKER_URL: redis://:legal_redis_2024!@redis:6379/2
      AIRFLOW__CORE__FERNET_KEY: 'Zv8eB7X0eH1K5k8F7yP9mN3wQ6tR2sA1dG4hJ8kL0oM='
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW_CONN_MYSQL_BLUE: 'mysql://legal_user:legal_pass_2024!@mysql-blue:3306/legal_db'
      AIRFLOW_CONN_MYSQL_GREEN: 'mysql://legal_user:legal_pass_2024!@mysql-green:3307/legal_db'
      AIRFLOW_CONN_KAFKA: 'kafka://kafka1:9092,kafka2:9093,kafka3:9094'
    volumes:
      - ./src:/opt/airflow/dags
      - ./src/airflow/logs:/opt/airflow/logs
      - ./src/airflow/plugins:/opt/airflow/plugins
      - ./src/airflow/config:/opt/airflow/config
      - airflow_logs:/opt/airflow/logs
    command: scheduler
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - legal-network

  airflow-worker:
    image: custom-airflow:2.8.4
    container_name: airflow-worker
    hostname: airflow-worker
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow_pass_2024!@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: redis://:legal_redis_2024!@redis:6379/1
      AIRFLOW__CELERY__BROKER_URL: redis://:legal_redis_2024!@redis:6379/2
      AIRFLOW__CORE__FERNET_KEY: 'Zv8eB7X0eH1K5k8F7yP9mN3wQ6tR2sA1dG4hJ8kL0oM='
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
      AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
      AIRFLOW_CONN_MYSQL_BLUE: 'mysql://legal_user:legal_pass_2024!@mysql-blue:3306/legal_db'
      AIRFLOW_CONN_MYSQL_GREEN: 'mysql://legal_user:legal_pass_2024!@mysql-green:3307/legal_db'
      AIRFLOW_CONN_KAFKA: 'kafka://kafka1:9092,kafka2:9093,kafka3:9094'
    volumes:
      - ./src:/opt/airflow/dags
      - ./src/airflow/logs:/opt/airflow/logs
      - ./src/airflow/plugins:/opt/airflow/plugins
      - ./src/airflow/config:/opt/airflow/config
      - airflow_logs:/opt/airflow/logs
    command: celery worker
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", 'celery --app airflow.executors.celery_executor.app inspect ping -d "celery@$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - legal-network

  airflow-flower:
    image: custom-airflow:2.8.4
    container_name: airflow-flower
    hostname: airflow-flower
    depends_on:
      redis:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow_pass_2024!@postgres/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: redis://:legal_redis_2024!@redis:6379/1
      AIRFLOW__CELERY__BROKER_URL: redis://:legal_redis_2024!@redis:6379/2
      AIRFLOW__CORE__FERNET_KEY: 'Zv8eB7X0eH1K5k8F7yP9mN3wQ6tR2sA1dG4hJ8kL0oM='
    volumes:
      - ./src/airflow/logs:/opt/airflow/logs
      - airflow_logs:/opt/airflow/logs
    ports:
      - "5555:5555"
    command: celery flower
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:5555/"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - legal-network

  # ==================== Monitoring ====================
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
      - '--storage.tsdb.retention.time=30d'
    restart: unless-stopped
    networks:
      - legal-network

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: legal_grafana_2024!
      GF_INSTALL_PLUGINS: grafana-clock-panel,grafana-simple-json-datasource
    volumes:
      - grafana_data:/var/lib/grafana
      - ./docker/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./docker/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    restart: unless-stopped
    depends_on:
      - prometheus
    networks:
      - legal-network

# ==================== Networks ====================
networks:
  legal-network:
    driver: bridge
    name: legal-network

# ==================== Volumes ====================
volumes:
  # Kafka & Zookeeper
  zookeeper_data:
    name: zookeeper_data
  zookeeper_logs:
    name: zookeeper_logs
  kafka1_data:
    name: kafka1_data
  kafka2_data:
    name: kafka2_data
  kafka3_data:
    name: kafka3_data
  
  # MySQL Blue-Green
  mysql_blue_data:
    name: mysql_blue_data
  mysql_blue_logs:
    name: mysql_blue_logs
  mysql_green_data:
    name: mysql_green_data
  mysql_green_logs:
    name: mysql_green_logs
  
  # Redis
  redis_data:
    name: redis_data
  
  # Airflow
  postgres_data:
    name: postgres_data
  airflow_logs:
    name: airflow_logs
  
  # Monitoring
  prometheus_data:
    name: prometheus_data
  grafana_data:
    name: grafana_data
