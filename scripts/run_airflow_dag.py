#!/usr/bin/env python3
"""
Airflow DAG ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (Docker ì—°ë™)
Docker Composeë¡œ ì‹¤í–‰ëœ Airflowì™€ ì—°ë™í•˜ì—¬ DAG ê´€ë¦¬
batch_monitor.pyë¥¼ í™œìš©í•œ ì‹¤ì œ ë°°ì¹˜ ì‘ì—… ì‹¤í–‰
"""
import argparse
import sys
import os
import json
import subprocess
import requests
from datetime import datetime, timedelta, date
from typing import Dict, Any, Optional
from src.logging_config import get_logger

# batch_monitor ëª¨ë“ˆ ì„í¬íŠ¸
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from batch_monitor import BatchMonitor

logger = get_logger(__name__)

# Docker Airflow ì„¤ì •
AIRFLOW_BASE_URL = "http://localhost:8090"
AIRFLOW_USERNAME = "airflow"
AIRFLOW_PASSWORD = "airflow_admin_2024!"

def _check_airflow_connection() -> bool:
    """Airflow ì—°ê²° ìƒíƒœ í™•ì¸"""
    try:
        response = requests.get(f"{AIRFLOW_BASE_URL}/health", timeout=5)
        if response.status_code == 200:
            logger.info("Airflow ì—°ê²° ì„±ê³µ")
            return True
        else:
            logger.error(f"Airflow ì—°ê²° ì‹¤íŒ¨: HTTP {response.status_code}")
            return False
    except Exception as e:
        logger.error(f"Airflow ì—°ê²° ì‹¤íŒ¨: {str(e)}")
        return False

def _ensure_dag_exists(dag_id: str) -> bool:
    """DAG ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ìƒì„±"""
    try:
        # DAG ëª©ë¡ ì¡°íšŒ
        response = requests.get(
            f"{AIRFLOW_BASE_URL}/api/v1/dags/{dag_id}",
            auth=(AIRFLOW_USERNAME, AIRFLOW_PASSWORD),
            timeout=10
        )
        
        if response.status_code == 200:
            logger.info(f"DAG '{dag_id}' ì¡´ì¬ í™•ì¸")
            return True
        elif response.status_code == 404:
            logger.warning(f"DAG '{dag_id}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. DAG íŒŒì¼ì„ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤.")
            return _create_sample_dag(dag_id)
        else:
            logger.error(f"DAG í™•ì¸ ì‹¤íŒ¨: HTTP {response.status_code}")
            return False
    except Exception as e:
        logger.error(f"DAG í™•ì¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

def _create_sample_dag(dag_id: str) -> bool:
    """ì‹¤ì œ batch_monitor.pyë¥¼ ì‚¬ìš©í•˜ëŠ” DAG íŒŒì¼ ìƒì„±"""
    try:
        # ë°°ì¹˜ ëª¨ë‹ˆí„° ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ
        batch_monitor_path = os.path.join(os.path.dirname(__file__), "batch_monitor.py")
        
        dag_content = f'''#!/usr/bin/env python3
"""
ë²•ì œì²˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ DAG - batch_monitor.py ì—°ë™
"""
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash import BashOperator
from airflow.operators.python import PythonOperator
import subprocess
import os

# ë°°ì¹˜ ëª¨ë‹ˆí„° ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ
BATCH_MONITOR_PATH = "{batch_monitor_path}"
PROJECT_ROOT = "{os.path.dirname(os.path.dirname(__file__))}"

# DAG ê¸°ë³¸ ì„¤ì •
default_args = {{
    'owner': 'legal-data-team',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=2),
}}

# DAG ì •ì˜
dag = DAG(
    dag_id='{dag_id}',
    default_args=default_args,
    description='ë²•ì œì²˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ - batch_monitor.py í™œìš©',
    schedule_interval='@daily',
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['legal', 'data-pipeline', 'batch-monitor'],
)

def check_system_health(**context):
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ - batch_monitor.py ì‚¬ìš©"""
    try:
        cmd = ["uv", "run", "python", BATCH_MONITOR_PATH, "status"]
        result = subprocess.run(cmd, cwd=PROJECT_ROOT, capture_output=True, text=True, timeout=300)
        
        if result.returncode == 0:
            print("âœ… ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì™„ë£Œ")
            print(result.stdout)
            return True
        else:
            print("âŒ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨")
            print(result.stderr)
            raise Exception("ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨")
            
    except subprocess.TimeoutExpired:
        raise Exception("ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ íƒ€ì„ì•„ì›ƒ")
    except Exception as e:
        raise Exception(f"ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {{str(e)}}")

def run_incremental_update(**context):
    """ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹¤í–‰ - batch_monitor.py ì‚¬ìš©"""
    try:
        # DAG ì„¤ì •ì—ì„œ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        dag_run = context.get('dag_run')
        conf = dag_run.conf if dag_run else {{}}
        
        use_api = conf.get('use_api', False)
        target_date = conf.get('target_date')
        
        cmd = ["uv", "run", "python", BATCH_MONITOR_PATH, "run", "incremental_update"]
        
        if use_api:
            cmd.append("--use-api")
        if target_date:
            cmd.extend(["--target-date", target_date])
        
        print(f"ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹¤í–‰: {{' '.join(cmd)}}")
        result = subprocess.run(cmd, cwd=PROJECT_ROOT, capture_output=True, text=True, timeout=1800)
        
        if result.returncode == 0:
            print("âœ… ì¦ë¶„ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            print(result.stdout)
            return True
        else:
            print("âŒ ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
            print(result.stderr)
            raise Exception("ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
            
    except subprocess.TimeoutExpired:
        raise Exception("ì¦ë¶„ ì—…ë°ì´íŠ¸ íƒ€ì„ì•„ì›ƒ (30ë¶„)")
    except Exception as e:
        raise Exception(f"ì¦ë¶„ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {{str(e)}}")

def run_data_validation(**context):
    """ë°ì´í„° ê²€ì¦ ì‹¤í–‰ - batch_monitor.py ì‚¬ìš©"""
    try:
        # DAG ì„¤ì •ì—ì„œ íŒŒë¼ë¯¸í„° ê°€ì ¸ì˜¤ê¸°
        dag_run = context.get('dag_run')
        conf = dag_run.conf if dag_run else {{}}
        
        fix_issues = conf.get('fix_issues', False)
        
        cmd = ["uv", "run", "python", BATCH_MONITOR_PATH, "run", "validation"]
        
        if fix_issues:
            cmd.append("--fix")
        
        print(f"ë°ì´í„° ê²€ì¦ ì‹¤í–‰: {{' '.join(cmd)}}")
        result = subprocess.run(cmd, cwd=PROJECT_ROOT, capture_output=True, text=True, timeout=600)
        
        if result.returncode == 0:
            print("âœ… ë°ì´í„° ê²€ì¦ ì™„ë£Œ")
            print(result.stdout)
            return True
        else:
            print("âš ï¸ ë°ì´í„° ê²€ì¦ì—ì„œ ë¬¸ì œ ë°œê²¬")
            print(result.stdout)
            print(result.stderr)
            # ê²€ì¦ ì‹¤íŒ¨ëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ ì¤‘ë‹¨í•˜ì§€ ì•ŠìŒ (ê²½ê³ ë§Œ)
            return True
            
    except subprocess.TimeoutExpired:
        raise Exception("ë°ì´í„° ê²€ì¦ íƒ€ì„ì•„ì›ƒ (10ë¶„)")
    except Exception as e:
        raise Exception(f"ë°ì´í„° ê²€ì¦ ì˜¤ë¥˜: {{str(e)}}")

def send_notification(**context):
    """ì‘ì—… ì™„ë£Œ ì•Œë¦¼"""
    try:
        task_instance = context.get('task_instance')
        dag_run = context.get('dag_run')
        
        # ì´ì „ íƒœìŠ¤í¬ë“¤ì˜ ìƒíƒœ í™•ì¸
        upstream_tasks = task_instance.get_direct_relatives(upstream=True)
        failed_tasks = []
        
        for task in upstream_tasks:
            task_state = task_instance.xcom_pull(task_ids=task.task_id)
            if not task_state:
                failed_tasks.append(task.task_id)
        
        # ì•Œë¦¼ ë©”ì‹œì§€ ìƒì„±
        if failed_tasks:
            message = f"âš ï¸ ë°°ì¹˜ ì‘ì—… ì™„ë£Œ (ì¼ë¶€ ì‹¤íŒ¨)\\nì‹¤íŒ¨í•œ íƒœìŠ¤í¬: {{', '.join(failed_tasks)}}"
        else:
            message = f"âœ… ë°°ì¹˜ ì‘ì—… ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ"
        
        message += f"\\nì‹¤í–‰ ì‹œê°„: {{context.get('execution_date')}}"
        message += f"\\nDAG Run ID: {{dag_run.dag_run_id if dag_run else 'Unknown'}}"
        
        print(message)
        
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” Slack, ì´ë©”ì¼ ë“±ìœ¼ë¡œ ì•Œë¦¼ ì „ì†¡
        # from src.notifications.slack_service import send_slack_notification
        # send_slack_notification(message)
        
        return message
        
    except Exception as e:
        print(f"ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {{str(e)}}")
        return f"ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {{str(e)}}"

# íƒœìŠ¤í¬ ì •ì˜
health_check = PythonOperator(
    task_id='health_check',
    python_callable=check_system_health,
    dag=dag,
)

incremental_update = PythonOperator(
    task_id='incremental_update',
    python_callable=run_incremental_update,
    dag=dag,
)

data_validation = PythonOperator(
    task_id='data_validation',
    python_callable=run_data_validation,
    dag=dag,
    trigger_rule='none_failed',  # ì´ì „ íƒœìŠ¤í¬ê°€ ì‹¤íŒ¨í•´ë„ ì‹¤í–‰
)

notification = PythonOperator(
    task_id='notification',
    python_callable=send_notification,
    dag=dag,
    trigger_rule='all_done',  # ëª¨ë“  ì´ì „ íƒœìŠ¤í¬ê°€ ì™„ë£Œë˜ë©´ ì‹¤í–‰
)

# íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì •
health_check >> incremental_update >> data_validation >> notification
'''
        
        # DAG íŒŒì¼ ê²½ë¡œ
        dag_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'src', 'airflow', 'dags')
        os.makedirs(dag_dir, exist_ok=True)
        
        dag_file = os.path.join(dag_dir, f'{dag_id}.py')
        
        with open(dag_file, 'w', encoding='utf-8') as f:
            f.write(dag_content)
        
        logger.info(f"DAG íŒŒì¼ ìƒì„± ì™„ë£Œ: {dag_file}")
        
        # Airflowì—ì„œ DAG íŒŒì¼ì„ ì¸ì‹í•  ì‹œê°„ì„ ì¤Œ
        import time
        time.sleep(10)
        
        return True
        
    except Exception as e:
        logger.error(f"DAG íŒŒì¼ ìƒì„± ì‹¤íŒ¨: {str(e)}")
        return False

def trigger_dag(dag_id: str, execution_date: Optional[str] = None, 
                conf: Optional[Dict[str, Any]] = None) -> bool:
    """DAG ì‹¤í–‰ íŠ¸ë¦¬ê±°"""
    try:
        if not _check_airflow_connection():
            logger.error("Airflowì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        if not _ensure_dag_exists(dag_id):
            logger.error(f"DAG '{dag_id}'ë¥¼ í™•ì¸/ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
        
        # DAG ì‹¤í–‰
        trigger_data = {
            "dag_run_id": f"manual_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
            "logical_date": execution_date or datetime.now().isoformat(),
            "conf": conf or {}
        }
        
        response = requests.post(
            f"{AIRFLOW_BASE_URL}/api/v1/dags/{dag_id}/dagRuns",
            auth=(AIRFLOW_USERNAME, AIRFLOW_PASSWORD),
            json=trigger_data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            logger.info(f"DAG ì‹¤í–‰ íŠ¸ë¦¬ê±° ì„±ê³µ: {result.get('dag_run_id')}")
            return True
        else:
            logger.error(f"DAG ì‹¤í–‰ íŠ¸ë¦¬ê±° ì‹¤íŒ¨: HTTP {response.status_code}, {response.text}")
            return False
        
    except Exception as e:
        logger.error(f"DAG ì‹¤í–‰ íŠ¸ë¦¬ê±° ì‹¤íŒ¨: {str(e)}")
        return False

def get_dag_status(dag_id: str, execution_date: Optional[str] = None) -> Dict[str, Any]:
    """DAG ì‹¤í–‰ ìƒíƒœ ì¡°íšŒ"""
    try:
        if not _check_airflow_connection():
            return {'status': 'error', 'message': 'Airflow ì—°ê²° ì‹¤íŒ¨'}
        
        # ìµœê·¼ DAG ì‹¤í–‰ ì¡°íšŒ
        response = requests.get(
            f"{AIRFLOW_BASE_URL}/api/v1/dags/{dag_id}/dagRuns",
            auth=(AIRFLOW_USERNAME, AIRFLOW_PASSWORD),
            params={'limit': 1, 'order_by': '-logical_date'},
            timeout=10
        )
        
        if response.status_code == 200:
            data = response.json()
            dag_runs = data.get('dag_runs', [])
            
            if not dag_runs:
                return {'status': 'not_found', 'message': 'DAG ì‹¤í–‰ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'}
            
            dag_run = dag_runs[0]
            return {
                'dag_id': dag_id,
                'dag_run_id': dag_run.get('dag_run_id'),
                'logical_date': dag_run.get('logical_date'),
                'state': dag_run.get('state'),
                'start_date': dag_run.get('start_date'),
                'end_date': dag_run.get('end_date'),
                'external_trigger': dag_run.get('external_trigger', False)
            }
        else:
            return {'status': 'error', 'message': f'HTTP {response.status_code}'}
        
    except Exception as e:
        logger.error(f"DAG ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return {'status': 'error', 'message': str(e)}

def list_recent_runs(dag_id: str, limit: int = 10) -> list[Dict[str, Any]]:
    """ìµœê·¼ DAG ì‹¤í–‰ ëª©ë¡ ì¡°íšŒ"""
    try:
        if not _check_airflow_connection():
            return []
        
        response = requests.get(
            f"{AIRFLOW_BASE_URL}/api/v1/dags/{dag_id}/dagRuns",
            auth=(AIRFLOW_USERNAME, AIRFLOW_PASSWORD),
            params={'limit': limit, 'order_by': '-logical_date'},
            timeout=10
        )
        
        if response.status_code == 200:
            data = response.json()
            return data.get('dag_runs', [])
        else:
            logger.error(f"DAG ì‹¤í–‰ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: HTTP {response.status_code}")
            return []
        
    except Exception as e:
        logger.error(f"ìµœê·¼ ì‹¤í–‰ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        return []

def run_incremental_update(target_date: Optional[str] = None, 
                          force: bool = False, use_api: bool = False) -> bool:
    """ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹¤í–‰ - batch_monitor.py ì§ì ‘ ì‚¬ìš©"""
    logger.info("batch_monitor.pyë¥¼ í†µí•œ ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹¤í–‰ ì‹œì‘", 
                target_date=target_date, force=force, use_api=use_api)
    
    try:
        # BatchMonitor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        monitor = BatchMonitor()
        
        # ì§ì ‘ ë°°ì¹˜ ì‘ì—… ì‹¤í–‰
        kwargs = {
            'use_api': use_api,
            'target_date': target_date
        }
        
        success = monitor.run_batch_job('incremental_update', **kwargs)
        
        if success:
            logger.info("batch_monitor.pyë¥¼ í†µí•œ ì¦ë¶„ ì—…ë°ì´íŠ¸ ì„±ê³µ")
            
            # ì¶”ê°€ë¡œ Airflow DAGë„ íŠ¸ë¦¬ê±°
            conf = {
                'use_api': use_api,
                'target_date': target_date,
                'force_execution': force,
                'triggered_by': 'batch_monitor_integration'
            }
            
            dag_success = trigger_dag('legal_data_pipeline', target_date, conf)
            if dag_success:
                logger.info("Airflow DAGë„ ì„±ê³µì ìœ¼ë¡œ íŠ¸ë¦¬ê±°ë¨")
            else:
                logger.warning("Airflow DAG íŠ¸ë¦¬ê±°ëŠ” ì‹¤íŒ¨í–ˆì§€ë§Œ ë°°ì¹˜ ì‘ì—… ìì²´ëŠ” ì„±ê³µ")
            
            return True
        else:
            logger.error("batch_monitor.pyë¥¼ í†µí•œ ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
            return False
            
    except Exception as e:
        logger.error(f"batch_monitor.py ì—°ë™ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

def run_health_check() -> Dict[str, Any]:
    """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì‹¤í–‰ - batch_monitor.py í™œìš©"""
    logger.info("batch_monitor.pyë¥¼ í†µí•œ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì‹œì‘")
    
    try:
        # BatchMonitor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        monitor = BatchMonitor()
        
        result = {
            'timestamp': datetime.now().isoformat(),
            'batch_monitor_available': True,
            'airflow_status': {
                'healthy': _check_airflow_connection(),
                'url': AIRFLOW_BASE_URL
            }
        }
        
        # batch_monitor.pyì˜ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ê¸°ëŠ¥ í™œìš©
        try:
            # ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒëŠ” ì¶œë ¥ ê¸°ë°˜ì´ë¯€ë¡œ subprocessë¡œ ì‹¤í–‰
            batch_monitor_path = os.path.join(os.path.dirname(__file__), "batch_monitor.py")
            cmd = ["uv", "run", "python", batch_monitor_path, "status"]
            
            process_result = subprocess.run(
                cmd, 
                cwd=os.path.dirname(os.path.dirname(__file__)),
                capture_output=True, 
                text=True, 
                timeout=30
            )
            
            if process_result.returncode == 0:
                result['batch_system_status'] = {
                    'healthy': True,
                    'details': process_result.stdout
                }
            else:
                result['batch_system_status'] = {
                    'healthy': False,
                    'error': process_result.stderr
                }
                
        except subprocess.TimeoutExpired:
            result['batch_system_status'] = {
                'healthy': False,
                'error': 'ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ íƒ€ì„ì•„ì›ƒ'
            }
        except Exception as e:
            result['batch_system_status'] = {
                'healthy': False,
                'error': f'ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì˜¤ë¥˜: {str(e)}'
            }
        
        # ì „ì²´ ì‹œìŠ¤í…œ ìƒíƒœ íŒë‹¨
        result['overall_healthy'] = (
            result['airflow_status']['healthy'] and
            result['batch_system_status'].get('healthy', False)
        )
        
        logger.info("batch_monitor.pyë¥¼ í†µí•œ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì™„ë£Œ", result=result)
        return result
        
    except Exception as e:
        logger.error(f"batch_monitor.py ì—°ë™ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return {
            'timestamp': datetime.now().isoformat(),
            'batch_monitor_available': False,
            'error': str(e),
            'overall_healthy': False
        }

def run_full_load(use_api: bool = False, batch_size: int = 100) -> bool:
    """ì „ì²´ ë°ì´í„° ì ì¬ ì‹¤í–‰ - batch_monitor.py í™œìš©"""
    logger.info("batch_monitor.pyë¥¼ í†µí•œ ì „ì²´ ë°ì´í„° ì ì¬ ì‹¤í–‰ ì‹œì‘",
                use_api=use_api, batch_size=batch_size)
    
    try:
        # BatchMonitor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        monitor = BatchMonitor()
        
        # ì§ì ‘ ë°°ì¹˜ ì‘ì—… ì‹¤í–‰
        kwargs = {
            'use_api': use_api,
            'batch_size': batch_size
        }
        
        success = monitor.run_batch_job('full_load', **kwargs)
        
        if success:
            logger.info("batch_monitor.pyë¥¼ í†µí•œ ì „ì²´ ë°ì´í„° ì ì¬ ì„±ê³µ")
            return True
        else:
            logger.error("batch_monitor.pyë¥¼ í†µí•œ ì „ì²´ ë°ì´í„° ì ì¬ ì‹¤íŒ¨")
            return False
            
    except Exception as e:
        logger.error(f"batch_monitor.py ì—°ë™ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

def run_data_validation(fix_issues: bool = False) -> bool:
    """ë°ì´í„° ê²€ì¦ ì‹¤í–‰ - batch_monitor.py í™œìš©"""
    logger.info("batch_monitor.pyë¥¼ í†µí•œ ë°ì´í„° ê²€ì¦ ì‹¤í–‰ ì‹œì‘", fix_issues=fix_issues)
    
    try:
        # BatchMonitor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        monitor = BatchMonitor()
        
        # ì§ì ‘ ë°°ì¹˜ ì‘ì—… ì‹¤í–‰
        kwargs = {
            'fix_issues': fix_issues
        }
        
        success = monitor.run_batch_job('validation', **kwargs)
        
        if success:
            logger.info("batch_monitor.pyë¥¼ í†µí•œ ë°ì´í„° ê²€ì¦ ì„±ê³µ")
            return True
        else:
            logger.warning("batch_monitor.pyë¥¼ í†µí•œ ë°ì´í„° ê²€ì¦ì—ì„œ ë¬¸ì œ ë°œê²¬")
            return False
            
    except Exception as e:
        logger.error(f"batch_monitor.py ì—°ë™ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ë²•ì œì²˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ Airflow DAG ê´€ë¦¬ (batch_monitor.py ì—°ë™)')
    
    subparsers = parser.add_subparsers(dest='command', help='ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹ì–´')
    
    # ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹¤í–‰
    incremental_parser = subparsers.add_parser('incremental', help='ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹¤í–‰')
    incremental_parser.add_argument('--date', type=str, help='ëŒ€ìƒ ë‚ ì§œ (YYYY-MM-DD)')
    incremental_parser.add_argument('--force', action='store_true', help='ê°•ì œ ì‹¤í–‰')
    incremental_parser.add_argument('--use-api', action='store_true', help='ì‹¤ì œ API ì‚¬ìš©')
    
    # ì „ì²´ ë°ì´í„° ì ì¬ ì‹¤í–‰
    fullload_parser = subparsers.add_parser('fullload', help='ì „ì²´ ë°ì´í„° ì ì¬ ì‹¤í–‰')
    fullload_parser.add_argument('--use-api', action='store_true', help='ì‹¤ì œ API ì‚¬ìš©')
    fullload_parser.add_argument('--batch-size', type=int, default=100, help='ë°°ì¹˜ í¬ê¸°')
    
    # ë°ì´í„° ê²€ì¦ ì‹¤í–‰
    validation_parser = subparsers.add_parser('validation', help='ë°ì´í„° ê²€ì¦ ì‹¤í–‰')
    validation_parser.add_argument('--fix', action='store_true', help='ë¬¸ì œ ìë™ ìˆ˜ì •')
    
    # DAG ìƒíƒœ ì¡°íšŒ
    status_parser = subparsers.add_parser('status', help='DAG ì‹¤í–‰ ìƒíƒœ ì¡°íšŒ')
    status_parser.add_argument('--dag-id', type=str, default='legal_data_pipeline', help='DAG ID')
    status_parser.add_argument('--date', type=str, help='ì‹¤í–‰ ë‚ ì§œ (YYYY-MM-DD)')
    
    # ìµœê·¼ ì‹¤í–‰ ëª©ë¡
    list_parser = subparsers.add_parser('list', help='ìµœê·¼ ì‹¤í–‰ ëª©ë¡ ì¡°íšŒ')
    list_parser.add_argument('--dag-id', type=str, default='legal_data_pipeline', help='DAG ID')
    list_parser.add_argument('--limit', type=int, default=10, help='ì¡°íšŒí•  ì‹¤í–‰ ê°œìˆ˜')
    
    # ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
    health_parser = subparsers.add_parser('health', help='ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸')
    
    # DAG íŠ¸ë¦¬ê±°
    trigger_parser = subparsers.add_parser('trigger', help='DAG ìˆ˜ë™ ì‹¤í–‰')
    trigger_parser.add_argument('--dag-id', type=str, default='legal_data_pipeline', help='DAG ID')
    trigger_parser.add_argument('--date', type=str, help='ì‹¤í–‰ ë‚ ì§œ (YYYY-MM-DD)')
    trigger_parser.add_argument('--conf', type=str, help='ì„¤ì • JSON')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    try:
        if args.command == 'incremental':
            success = run_incremental_update(args.date, args.force, args.use_api)
            if success:
                print("âœ… ì¦ë¶„ ì—…ë°ì´íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                print(f"ğŸ”— Airflow UIì—ì„œ í™•ì¸: {AIRFLOW_BASE_URL}")
            else:
                print("âŒ ì¦ë¶„ ì—…ë°ì´íŠ¸ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                sys.exit(1)
        
        elif args.command == 'fullload':
            success = run_full_load(args.use_api, args.batch_size)
            if success:
                print("âœ… ì „ì²´ ë°ì´í„° ì ì¬ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                print("âŒ ì „ì²´ ë°ì´í„° ì ì¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                sys.exit(1)
        
        elif args.command == 'validation':
            success = run_data_validation(args.fix)
            if success:
                print("âœ… ë°ì´í„° ê²€ì¦ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                print("âš ï¸ ë°ì´í„° ê²€ì¦ì—ì„œ ë¬¸ì œê°€ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
                sys.exit(1)
        
        elif args.command == 'status':
            status = get_dag_status(args.dag_id, args.date)
            print(json.dumps(status, indent=2, ensure_ascii=False))
        
        elif args.command == 'list':
            runs = list_recent_runs(args.dag_id, args.limit)
            print(json.dumps(runs, indent=2, ensure_ascii=False))
        
        elif args.command == 'health':
            health = run_health_check()
            print(json.dumps(health, indent=2, ensure_ascii=False))
            
            if not health.get('overall_healthy', False):
                sys.exit(1)
        
        elif args.command == 'trigger':
            conf = json.loads(args.conf) if args.conf else None
            success = trigger_dag(args.dag_id, args.date, conf)
            if success:
                print(f"âœ… DAG '{args.dag_id}'ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
                print(f"ğŸ”— Airflow UIì—ì„œ í™•ì¸: {AIRFLOW_BASE_URL}")
            else:
                print(f"âŒ DAG '{args.dag_id}' ì‹œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                sys.exit(1)
        
    except KeyboardInterrupt:
        print("\nì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        sys.exit(1)

if __name__ == '__main__':
    main()
