#!/usr/bin/env python3
"""
Kafka íŒŒí‹°ì…˜ ìµœì í™” í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ìƒˆë¡œìš´ íŒŒí‹°ì…”ë‹ ì „ëµì˜ íš¨ê³¼ë¥¼ ì‹œë®¬ë ˆì´ì…˜í•˜ê³  ê²€ì¦
"""
import asyncio
import time
import json
from datetime import datetime, date
from typing import Dict, List, Any, Tuple
import hashlib
import random

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.logging_config import get_logger

logger = get_logger(__name__)

class PartitionSimulator:
    """íŒŒí‹°ì…˜ ë¶„ì‚° ì‹œë®¬ë ˆì´í„°"""
    
    def __init__(self, partition_count: int):
        self.partition_count = partition_count
        self.partitions = {i: [] for i in range(partition_count)}
    
    def add_message(self, key: str, message_size: int = 1024):
        """ë©”ì‹œì§€ë¥¼ íŒŒí‹°ì…˜ì— ì¶”ê°€"""
        # í•´ì‹œ ê¸°ë°˜ íŒŒí‹°ì…˜ í• ë‹¹
        hash_value = int(hashlib.md5(key.encode()).hexdigest(), 16)
        partition_id = hash_value % self.partition_count
        
        self.partitions[partition_id].append({
            'key': key,
            'size': message_size,
            'timestamp': time.time()
        })
        
        return partition_id
    
    def get_distribution_stats(self) -> Dict[str, Any]:
        """íŒŒí‹°ì…˜ ë¶„ì‚° í†µê³„"""
        sizes = [len(self.partitions[i]) for i in range(self.partition_count)]
        total_messages = sum(sizes)
        
        if total_messages == 0:
            return {
                'total_messages': 0,
                'avg_per_partition': 0,
                'max_partition_size': 0,
                'min_partition_size': 0,
                'variance': 0,
                'distribution_score': 100
            }
        
        avg_size = total_messages / self.partition_count
        variance = sum((size - avg_size) ** 2 for size in sizes) / self.partition_count
        
        # ë¶„ì‚° ì ìˆ˜ ê³„ì‚° (0-100, 100ì´ ê°€ì¥ ê· ë“±í•œ ë¶„ì‚°)
        max_possible_variance = (total_messages ** 2) / 4  # ìµœì•…ì˜ ê²½ìš°
        distribution_score = max(0, 100 - (variance / max_possible_variance) * 100)
        
        return {
            'total_messages': total_messages,
            'avg_per_partition': avg_size,
            'max_partition_size': max(sizes),
            'min_partition_size': min(sizes),
            'variance': variance,
            'distribution_score': distribution_score,
            'partition_sizes': sizes
        }

class LegacyPartitionStrategy:
    """ê¸°ì¡´ íŒŒí‹°ì…˜ ì „ëµ (ê°œì„  ì „)"""
    
    @staticmethod
    def get_law_partition_key(law_id: str, **kwargs) -> str:
        """ë²•ë ¹ ì´ë²¤íŠ¸ íŒŒí‹°ì…˜ í‚¤ (ê¸°ì¡´ ë°©ì‹)"""
        return law_id
    
    @staticmethod
    def get_content_partition_key(law_id: str, **kwargs) -> str:
        """ë³¸ë¬¸ ì´ë²¤íŠ¸ íŒŒí‹°ì…˜ í‚¤ (ê¸°ì¡´ ë°©ì‹)"""
        return law_id
    
    @staticmethod
    def get_article_partition_key(law_master_no: str, article_no: int = None, **kwargs) -> str:
        """ì¡°í•­ ì´ë²¤íŠ¸ íŒŒí‹°ì…˜ í‚¤ (ê¸°ì¡´ ë°©ì‹)"""
        return law_master_no

class OptimizedPartitionStrategy:
    """ìµœì í™”ëœ íŒŒí‹°ì…˜ ì „ëµ (ê°œì„  í›„)"""
    
    @staticmethod
    def get_law_partition_key(law_id: str, ministry_code: int = None, **kwargs) -> str:
        """ë²•ë ¹ ì´ë²¤íŠ¸ íŒŒí‹°ì…˜ í‚¤ (ê°œì„ ëœ ë°©ì‹)"""
        if ministry_code:
            # ì†Œê´€ë¶€ì²˜ ê¸°ë°˜ ë¶„ì‚°
            return f"ministry_{ministry_code % 3}"
        
        # ì‹œê°„ ê¸°ë°˜ ë¶„ì‚°
        time_bucket = int(time.time() / 3600)
        combined_key = f"{law_id}_{time_bucket}"
        hash_value = int(hashlib.md5(combined_key.encode()).hexdigest(), 16)
        return str(hash_value % 1000000)
    
    @staticmethod
    def get_content_partition_key(law_id: str, content_size: int = 10000, **kwargs) -> str:
        """ë³¸ë¬¸ ì´ë²¤íŠ¸ íŒŒí‹°ì…˜ í‚¤ (ê°œì„ ëœ ë°©ì‹)"""
        # ë³¸ë¬¸ í¬ê¸° ê¸°ë°˜ ë¶„ì‚°
        size_bucket = "large" if content_size > 50000 else "medium" if content_size > 10000 else "small"
        combined_key = f"{law_id}_{size_bucket}"
        hash_value = int(hashlib.md5(combined_key.encode()).hexdigest(), 16)
        return str(hash_value % 1000000)
    
    @staticmethod
    def get_article_partition_key(law_master_no: str, article_no: int = None, 
                                  ministry_code: int = None, **kwargs) -> str:
        """ì¡°í•­ ì´ë²¤íŠ¸ íŒŒí‹°ì…˜ í‚¤ (ê°œì„ ëœ ë°©ì‹)"""
        if law_master_no and article_no:
            # ê´€ë ¨ ì¡°í•­ ê·¸ë£¹í•‘ (10ê°œ ë‹¨ìœ„)
            article_group = article_no // 10
            
            if ministry_code:
                combined_key = f"ministry_{ministry_code}_group_{article_group}"
            else:
                combined_key = f"{law_master_no}_group_{article_group}"
                
            hash_value = int(hashlib.md5(combined_key.encode()).hexdigest(), 16)
            return str(hash_value % 1000000)
        
        return law_master_no

def generate_test_data() -> List[Dict[str, Any]]:
    """í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±"""
    test_data = []
    
    # ì†Œê´€ë¶€ì²˜ë³„ ë²•ë ¹ ë¶„í¬ (ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ê°€ì •)
    ministry_distribution = {
        101: 500,   # ê¸°íšì¬ì •ë¶€ (ë§ìŒ)
        201: 300,   # êµìœ¡ë¶€
        301: 800,   # ë²•ë¬´ë¶€ (ë§¤ìš° ë§ìŒ)
        401: 150,   # êµ­ë°©ë¶€
        501: 200,   # í–‰ì •ì•ˆì „ë¶€
        601: 100,   # ë¬¸í™”ì²´ìœ¡ê´€ê´‘ë¶€
        701: 180,   # ë†ë¦¼ì¶•ì‚°ì‹í’ˆë¶€
        801: 120,   # ì‚°ì—…í†µìƒìì›ë¶€
        901: 90,    # ë³´ê±´ë³µì§€ë¶€
        1001: 70    # í™˜ê²½ë¶€
    }
    
    law_id_counter = 1
    
    for ministry_code, law_count in ministry_distribution.items():
        for _ in range(law_count):
            law_id = str(law_id_counter)
            law_master_no = str(law_id_counter * 100 + random.randint(1, 99))
            
            # ë²•ë ¹ë³„ ì¡°í•­ ìˆ˜ (ì •ê·œë¶„í¬, í‰ê·  25ê°œ)
            article_count = max(1, int(random.gauss(25, 10)))
            
            # ë²•ë ¹ ë³¸ë¬¸ í¬ê¸° (ê°€ë³€ì )
            content_size = random.choice([
                random.randint(5000, 15000),   # ì†Œê·œëª¨ ë²•ë ¹ (60%)
                random.randint(15000, 50000),  # ì¤‘ê°„ ê·œëª¨ ë²•ë ¹ (30%)
                random.randint(50000, 200000)  # ëŒ€ê·œëª¨ ë²•ë ¹ (10%)
            ]) if random.random() < 0.4 else random.randint(5000, 15000)
            
            law_data = {
                'law_id': law_id,
                'law_master_no': law_master_no,
                'ministry_code': ministry_code,
                'article_count': article_count,
                'content_size': content_size
            }
            
            test_data.append(law_data)
            law_id_counter += 1
    
    return test_data

def simulate_partition_distribution(test_data: List[Dict[str, Any]], 
                                  strategy_class, partition_counts: Dict[str, int]) -> Dict[str, Any]:
    """íŒŒí‹°ì…˜ ë¶„ì‚° ì‹œë®¬ë ˆì´ì…˜"""
    
    # ê° í† í”½ë³„ ì‹œë®¬ë ˆì´í„° ì´ˆê¸°í™”
    simulators = {
        'law_events': PartitionSimulator(partition_counts['law_events']),
        'content_events': PartitionSimulator(partition_counts['content_events']),
        'article_events': PartitionSimulator(partition_counts['article_events'])
    }
    
    # ë°ì´í„° ì²˜ë¦¬
    for law_data in test_data:
        # 1. ë²•ë ¹ ì´ë²¤íŠ¸
        law_key = strategy_class.get_law_partition_key(**law_data)
        simulators['law_events'].add_message(law_key, 1024)  # í‰ê·  1KB
        
        # 2. ë³¸ë¬¸ ì´ë²¤íŠ¸  
        content_key = strategy_class.get_content_partition_key(**law_data)
        simulators['content_events'].add_message(content_key, law_data['content_size'])
        
        # 3. ì¡°í•­ ì´ë²¤íŠ¸ (ì¡°í•­ ìˆ˜ë§Œí¼)
        for article_no in range(1, law_data['article_count'] + 1):
            article_key = strategy_class.get_article_partition_key(
                law_data['law_master_no'], 
                article_no=article_no,
                ministry_code=law_data['ministry_code']
            )
            simulators['article_events'].add_message(article_key, 512)  # í‰ê·  512B
    
    # í†µê³„ ìˆ˜ì§‘
    results = {}
    for topic, simulator in simulators.items():
        results[topic] = simulator.get_distribution_stats()
    
    return results

def compare_strategies():
    """ì „ëµ ë¹„êµ"""
    print("ğŸ”„ Kafka íŒŒí‹°ì…˜ ì „ëµ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    print("ğŸ“Š í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
    test_data = generate_test_data()
    print(f"âœ… ì´ {len(test_data)}ê°œ ë²•ë ¹ ë°ì´í„° ìƒì„± ì™„ë£Œ")
    
    # íŒŒí‹°ì…˜ ìˆ˜ ì„¤ì •
    legacy_partitions = {
        'law_events': 6,      # ê¸°ì¡´ ì„¤ì •
        'content_events': 4,  # ê¸°ì¡´ ì„¤ì •
        'article_events': 8   # ê¸°ì¡´ ì„¤ì •
    }
    
    optimized_partitions = {
        'law_events': 3,      # ìµœì í™”ëœ ì„¤ì •
        'content_events': 6,  # ìµœì í™”ëœ ì„¤ì •  
        'article_events': 12  # ìµœì í™”ëœ ì„¤ì •
    }
    
    print("\nğŸ” ê¸°ì¡´ ì „ëµ ì‹œë®¬ë ˆì´ì…˜...")
    legacy_results = simulate_partition_distribution(
        test_data, LegacyPartitionStrategy, legacy_partitions
    )
    
    print("âš¡ ìµœì í™”ëœ ì „ëµ ì‹œë®¬ë ˆì´ì…˜...")
    optimized_results = simulate_partition_distribution(
        test_data, OptimizedPartitionStrategy, optimized_partitions
    )
    
    # ê²°ê³¼ ë¹„êµ
    print("\nğŸ“ˆ ê²°ê³¼ ë¹„êµ")
    print("=" * 60)
    
    for topic in ['law_events', 'content_events', 'article_events']:
        legacy = legacy_results[topic]
        optimized = optimized_results[topic]
        
        print(f"\nğŸ“‹ {topic.replace('_', ' ').title()}")
        print("-" * 40)
        print(f"íŒŒí‹°ì…˜ ìˆ˜:")
        print(f"  ê¸°ì¡´:     {legacy_partitions[topic]}ê°œ")
        print(f"  ìµœì í™”:   {optimized_partitions[topic]}ê°œ")
        
        print(f"ì´ ë©”ì‹œì§€ ìˆ˜: {legacy['total_messages']:,}ê°œ")
        
        print(f"ë¶„ì‚° ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ):")
        print(f"  ê¸°ì¡´:     {legacy['distribution_score']:.1f}ì ")
        print(f"  ìµœì í™”:   {optimized['distribution_score']:.1f}ì ")
        print(f"  ê°œì„ :     {optimized['distribution_score'] - legacy['distribution_score']:+.1f}ì ")
        
        print(f"ë¶„ì‚° (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ):")
        print(f"  ê¸°ì¡´:     {legacy['variance']:.1f}")
        print(f"  ìµœì í™”:   {optimized['variance']:.1f}")
        
        improvement = ((legacy['variance'] - optimized['variance']) / legacy['variance'] * 100) if legacy['variance'] > 0 else 0
        print(f"  ê°œì„ :     {improvement:+.1f}%")
        
        print(f"íŒŒí‹°ì…˜ í¬ê¸° ë²”ìœ„:")
        print(f"  ê¸°ì¡´:     {legacy['min_partition_size']} ~ {legacy['max_partition_size']}")
        print(f"  ìµœì í™”:   {optimized['min_partition_size']} ~ {optimized['max_partition_size']}")
    
    # ì „ì²´ ìš”ì•½
    print(f"\nğŸ¯ ì „ì²´ ìš”ì•½")
    print("=" * 60)
    
    total_legacy_score = sum(legacy_results[topic]['distribution_score'] for topic in legacy_results) / 3
    total_optimized_score = sum(optimized_results[topic]['distribution_score'] for topic in optimized_results) / 3
    
    print(f"í‰ê·  ë¶„ì‚° ì ìˆ˜:")
    print(f"  ê¸°ì¡´ ì „ëµ:     {total_legacy_score:.1f}ì ")
    print(f"  ìµœì í™” ì „ëµ:   {total_optimized_score:.1f}ì ")
    print(f"  ì „ì²´ ê°œì„ :     {total_optimized_score - total_legacy_score:+.1f}ì ")
    
    if total_optimized_score > total_legacy_score:
        print(f"âœ… ìµœì í™”ëœ íŒŒí‹°ì…˜ ì „ëµì´ {total_optimized_score - total_legacy_score:.1f}ì  ë” ìš°ìˆ˜í•©ë‹ˆë‹¤!")
    else:
        print(f"âš ï¸ ê¸°ì¡´ ì „ëµì´ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.")
    
    # ìƒì„¸ ê²°ê³¼ë¥¼ JSONìœ¼ë¡œ ì €ì¥
    results = {
        'timestamp': datetime.now().isoformat(),
        'test_data_size': len(test_data),
        'legacy_strategy': {
            'partition_counts': legacy_partitions,
            'results': legacy_results,
            'avg_score': total_legacy_score
        },
        'optimized_strategy': {
            'partition_counts': optimized_partitions,
            'results': optimized_results,
            'avg_score': total_optimized_score
        },
        'improvement': {
            'score_improvement': total_optimized_score - total_legacy_score,
            'percentage_improvement': ((total_optimized_score - total_legacy_score) / total_legacy_score * 100) if total_legacy_score > 0 else 0
        }
    }
    
    # ê²°ê³¼ íŒŒì¼ ì €ì¥
    with open('partition_optimization_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    print(f"\nğŸ’¾ ìƒì„¸ ê²°ê³¼ê°€ 'partition_optimization_results.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return results

def generate_hot_partition_scenario():
    """Hot Partition ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”¥ Hot Partition ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # íŠ¹ì • ë¶€ì²˜ì— ì§‘ì¤‘ëœ ë°ì´í„° ìƒì„± (Hot Partition ìœ ë°œ)
    hot_ministry_data = []
    
    # ë²•ë¬´ë¶€(301)ì— 70% ì§‘ì¤‘
    for i in range(700):
        hot_ministry_data.append({
            'law_id': str(i + 10000),
            'law_master_no': str((i + 10000) * 100),
            'ministry_code': 301,  # ë²•ë¬´ë¶€ ì§‘ì¤‘
            'article_count': 30,
            'content_size': 30000
        })
    
    # ë‚˜ë¨¸ì§€ ë¶€ì²˜ì— 30% ë¶„ì‚°
    other_ministries = [101, 201, 401, 501]
    for i in range(300):
        ministry = random.choice(other_ministries)
        hot_ministry_data.append({
            'law_id': str(i + 20000),
            'law_master_no': str((i + 20000) * 100),
            'ministry_code': ministry,
            'article_count': 20,
            'content_size': 15000
        })
    
    print(f"ğŸ“Š Hot Partition í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(hot_ministry_data)}ê°œ ë²•ë ¹")
    print("   - ë²•ë¬´ë¶€(301): 70% ì§‘ì¤‘")
    print("   - ê¸°íƒ€ ë¶€ì²˜: 30% ë¶„ì‚°")
    
    # ê¸°ì¡´ ì „ëµìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜
    legacy_hot_results = simulate_partition_distribution(
        hot_ministry_data, LegacyPartitionStrategy, {'law_events': 6, 'content_events': 4, 'article_events': 8}
    )
    
    # ìµœì í™” ì „ëµìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜
    optimized_hot_results = simulate_partition_distribution(
        hot_ministry_data, OptimizedPartitionStrategy, {'law_events': 3, 'content_events': 6, 'article_events': 12}
    )
    
    print(f"\nğŸ”¥ Hot Partition ì €í•­ì„± í…ŒìŠ¤íŠ¸ - ë²•ë ¹ ì´ë²¤íŠ¸")
    print("-" * 50)
    
    legacy_law = legacy_hot_results['law_events']
    optimized_law = optimized_hot_results['law_events']
    
    print(f"ê¸°ì¡´ ì „ëµ (Hot Partition ìœ„í—˜):")
    print(f"  ìµœëŒ€ íŒŒí‹°ì…˜: {legacy_law['max_partition_size']} ë©”ì‹œì§€")
    print(f"  ìµœì†Œ íŒŒí‹°ì…˜: {legacy_law['min_partition_size']} ë©”ì‹œì§€")
    print(f"  ë¶„ì‚° ì ìˆ˜: {legacy_law['distribution_score']:.1f}ì ")
    
    print(f"ìµœì í™” ì „ëµ (Hot Partition ë°©ì§€):")
    print(f"  ìµœëŒ€ íŒŒí‹°ì…˜: {optimized_law['max_partition_size']} ë©”ì‹œì§€")
    print(f"  ìµœì†Œ íŒŒí‹°ì…˜: {optimized_law['min_partition_size']} ë©”ì‹œì§€")
    print(f"  ë¶„ì‚° ì ìˆ˜: {optimized_law['distribution_score']:.1f}ì ")
    
    hot_partition_improvement = optimized_law['distribution_score'] - legacy_law['distribution_score']
    print(f"Hot Partition ì €í•­ì„± ê°œì„ : {hot_partition_improvement:+.1f}ì ")
    
    if hot_partition_improvement > 10:
        print("âœ… ìµœì í™”ëœ ì „ëµì´ Hot Partitionì„ íš¨ê³¼ì ìœ¼ë¡œ ë°©ì§€í•©ë‹ˆë‹¤!")
    else:
        print("âš ï¸ Hot Partition ë°©ì§€ íš¨ê³¼ê°€ ì œí•œì ì…ë‹ˆë‹¤.")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Kafka íŒŒí‹°ì…˜ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    try:
        # ì¼ë°˜ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
        results = compare_strategies()
        
        # Hot Partition ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
        generate_hot_partition_scenario()
        
        print(f"\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print(f"â° ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
        return results
        
    except Exception as e:
        logger.error("íŒŒí‹°ì…˜ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨", error=str(e))
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
        return None

if __name__ == "__main__":
    main()
