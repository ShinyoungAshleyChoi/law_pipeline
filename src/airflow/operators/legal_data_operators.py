"""
ë²•ì œì²˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì „ìš© Airflow ì˜¤í¼ë ˆì´í„°
"""
from datetime import datetime, date, timedelta
from typing import Dict, Any, List, Optional
import uuid
import json

from airflow.models import BaseOperator
from airflow.utils.context import Context
from airflow.exceptions import AirflowException, AirflowSkipException
from airflow.utils.decorators import apply_defaults

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../../..'))

from src.api.client import api_client
# from src.processors.incremental_processor import incremental_processor  # ì‚­ì œë¨
from src.database.repository import LegalDataRepository
from src.notifications.slack_service import slack_service
from src.logging_config import get_logger

logger = get_logger(__name__)

class LegalAPIHealthCheckOperator(BaseOperator):
    """
    ë²•ì œì²˜ API ìƒíƒœ í™•ì¸ ì˜¤í¼ë ˆì´í„°
    """
    
    template_fields = ['max_response_time']
    ui_color = '#4CAF50'
    ui_fgcolor = '#FFFFFF'
    
    @apply_defaults
    def __init__(
        self,
        max_response_time: int = 10000,  # ìµœëŒ€ ì‘ë‹µ ì‹œê°„ (ms)
        fail_on_error: bool = True,      # ì˜¤ë¥˜ ì‹œ ì‹¤íŒ¨ ì²˜ë¦¬ ì—¬ë¶€
        **kwargs
    ):
        super().__init__(**kwargs)
        self.max_response_time = max_response_time
        self.fail_on_error = fail_on_error
    
    def execute(self, context: Context) -> Dict[str, Any]:
        """API ìƒíƒœ í™•ì¸ ì‹¤í–‰"""
        logger.info("ë²•ì œì²˜ API ìƒíƒœ í™•ì¸ ì‹œì‘")
        
        try:
            # API í—¬ìŠ¤ì²´í¬ ìˆ˜í–‰
            health_status = api_client.health_check()
            
            result = {
                'is_healthy': health_status.is_healthy,
                'response_time_ms': health_status.response_time_ms,
                'error_message': health_status.error_message,
                'check_time': datetime.now().isoformat(),
                'max_response_time': self.max_response_time
            }
            
            # ìƒíƒœ ê²€ì¦
            if not health_status.is_healthy:
                error_msg = f"API ìƒíƒœ ë¶ˆëŸ‰: {health_status.error_message}"
                logger.error(error_msg)
                
                if self.fail_on_error:
                    raise AirflowException(error_msg)
                else:
                    result['warning'] = error_msg
            
            # ì‘ë‹µ ì‹œê°„ ê²€ì¦
            if health_status.response_time_ms > self.max_response_time:
                warning_msg = f"API ì‘ë‹µ ì‹œê°„ ì´ˆê³¼: {health_status.response_time_ms}ms > {self.max_response_time}ms"
                logger.warning(warning_msg)
                result['warning'] = warning_msg
            
            logger.info("API ìƒíƒœ í™•ì¸ ì™„ë£Œ", result=result)
            return result
            
        except Exception as e:
            logger.error("API ìƒíƒœ í™•ì¸ ì‹¤íŒ¨", error=str(e))
            if self.fail_on_error:
                raise
            return {
                'is_healthy': False,
                'error_message': str(e),
                'check_time': datetime.now().isoformat()
            }

class LegalDataCollectionOperator(BaseOperator):
    """
    ë²•ì œì²˜ ë°ì´í„° ìˆ˜ì§‘ ì˜¤í¼ë ˆì´í„°
    """
    
    template_fields = ['target_date', 'batch_size']
    ui_color = '#2196F3'
    ui_fgcolor = '#FFFFFF'
    
    @apply_defaults
    def __init__(
        self,
        collection_type: str = 'incremental',  # 'incremental' or 'full' or 'target_date'
        target_date: Optional[str] = None,     # YYYY-MM-DD í˜•ì‹
        batch_size: int = 50,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.collection_type = collection_type
        self.target_date = target_date
        self.batch_size = batch_size
    
    def execute(self, context: Context) -> Dict[str, Any]:
        """ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
        logger.info("ë²•ì œì²˜ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘", 
                   collection_type=self.collection_type,
                   target_date=self.target_date)
        
        try:
            job_id = self._generate_job_id(context)
            
            if self.collection_type == 'incremental':
                return self._execute_incremental_collection(job_id, context)
            elif self.collection_type == 'target_date':
                return self._execute_target_date_collection(job_id, context)
            elif self.collection_type == 'full':
                return self._execute_full_collection(job_id, context)
            else:
                raise AirflowException(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ìˆ˜ì§‘ íƒ€ì…: {self.collection_type}")
                
        except Exception as e:
            logger.error("ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨", error=str(e))
            raise
    
    def _generate_job_id(self, context: Context) -> str:
        """ì‘ì—… ID ìƒì„±"""
        execution_date = context['execution_date']
        job_id = f"legal_collection_{execution_date.strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
        
        # XComì— ì €ì¥
        context['task_instance'].xcom_push(key='job_id', value=job_id)
        return job_id
    
    def _execute_incremental_collection(self, job_id: str, context: Context) -> Dict[str, Any]:
        """ì¦ë¶„ ìˆ˜ì§‘ ì‹¤í–‰"""
        logger.info("ì¦ë¶„ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘", job_id=job_id)
        
        # Mock ì¦ë¶„ ì—…ë°ì´íŠ¸ ì‹¤í–‰ (ì‹¤ì œ ë¡œì§ì€ ë³„ë„ êµ¬í˜„ í•„ìš”)
        all_stats = [{
            'target_date': date.today(),
            'total_laws_found': 0,
            'new_laws': 0,
            'updated_laws': 0,
            'error_count': 0
        }]
        
        # ê²°ê³¼ ì§‘ê³„
        total_stats = {
            'total_new_laws': sum(s.new_laws for s in all_stats),
            'total_updated_laws': sum(s.updated_laws for s in all_stats),
            'total_new_articles': sum(s.new_articles for s in all_stats),
            'total_updated_articles': sum(s.updated_articles for s in all_stats),
            'total_errors': sum(s.error_count for s in all_stats),
            'total_processing_time': sum(s.processing_time_seconds for s in all_stats),
            'processed_dates': len(all_stats)
        }
        
        result = {
            'job_id': job_id,
            'collection_type': 'incremental',
            'success': total_stats['total_errors'] == 0,
            'stats': total_stats,
            'daily_stats': [
                {
                    'target_date': stats.target_date.isoformat(),
                    'total_laws_found': stats.total_laws_found,
                    'new_laws': stats.new_laws,
                    'updated_laws': stats.updated_laws,
                    'new_articles': stats.new_articles,
                    'updated_articles': stats.updated_articles,
                    'error_count': stats.error_count,
                    'processing_time_seconds': stats.processing_time_seconds
                }
                for stats in all_stats
            ],
            'completion_time': datetime.now().isoformat()
        }
        
        logger.info("ì¦ë¶„ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ", result=result['stats'])
        return result
    
    def _execute_target_date_collection(self, job_id: str, context: Context) -> Dict[str, Any]:
        """íŠ¹ì • ë‚ ì§œ ìˆ˜ì§‘ ì‹¤í–‰"""
        if not self.target_date:
            raise AirflowException("target_dateê°€ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        target_date_obj = datetime.strptime(self.target_date, '%Y-%m-%d').date()
        logger.info("íŠ¹ì • ë‚ ì§œ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘", 
                   job_id=job_id, 
                   target_date=target_date_obj)
        
        # Mock ì¼ë³„ ì²˜ë¦¬ (ì‹¤ì œ ë¡œì§ì€ ë³„ë„ êµ¬í˜„ í•„ìš”)
        daily_stats = {
            'target_date': target_date_obj,
            'total_laws_found': 0,
            'new_laws': 0,
            'updated_laws': 0,
            'error_count': 0
        }
        
        result = {
            'job_id': job_id,
            'collection_type': 'target_date',
            'target_date': target_date_obj.isoformat(),
            'success': daily_stats.error_count == 0,
            'stats': {
                'total_laws_found': daily_stats.total_laws_found,
                'new_laws': daily_stats.new_laws,
                'updated_laws': daily_stats.updated_laws,
                'new_articles': daily_stats.new_articles,
                'updated_articles': daily_stats.updated_articles,
                'error_count': daily_stats.error_count,
                'processing_time_seconds': daily_stats.processing_time_seconds
            },
            'completion_time': datetime.now().isoformat()
        }
        
        logger.info("íŠ¹ì • ë‚ ì§œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ", result=result['stats'])
        return result
    
    def _execute_full_collection(self, job_id: str, context: Context) -> Dict[str, Any]:
        """ì „ì²´ ìˆ˜ì§‘ ì‹¤í–‰ (êµ¬í˜„ ì˜ˆì •)"""
        logger.warning("ì „ì²´ ìˆ˜ì§‘ì€ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        raise AirflowSkipException("ì „ì²´ ìˆ˜ì§‘ì€ ë³„ë„ DAGì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤")

class LegalDataValidationOperator(BaseOperator):
    """
    ë²•ì œì²˜ ë°ì´í„° ê²€ì¦ ì˜¤í¼ë ˆì´í„°
    """
    
    template_fields = ['validation_rules']
    ui_color = '#FF9800'
    ui_fgcolor = '#FFFFFF'
    
    @apply_defaults
    def __init__(
        self,
        validation_rules: Optional[Dict[str, Any]] = None,
        fail_on_validation_error: bool = True,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.validation_rules = validation_rules or {}
        self.fail_on_validation_error = fail_on_validation_error
    
    def execute(self, context: Context) -> Dict[str, Any]:
        """ë°ì´í„° ê²€ì¦ ì‹¤í–‰"""
        logger.info("ë°ì´í„° ê²€ì¦ ì‹œì‘")
        
        try:
            # ì´ì „ íƒœìŠ¤í¬ì—ì„œ ìˆ˜ì§‘ ê²°ê³¼ ì¡°íšŒ
            collection_result = context['task_instance'].xcom_pull(
                task_ids=self._get_collection_task_id(context)
            )
            
            if not collection_result:
                raise AirflowException("ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            
            # ê¸°ë³¸ ê²€ì¦ ìˆ˜í–‰
            validation_result = self._perform_basic_validation(collection_result)
            
            # ì¶”ê°€ ê²€ì¦ ê·œì¹™ ì ìš©
            if self.validation_rules:
                additional_validation = self._apply_validation_rules(
                    collection_result, self.validation_rules
                )
                validation_result.update(additional_validation)
            
            # ê²€ì¦ ê²°ê³¼ í‰ê°€
            is_valid = validation_result.get('is_valid', True)
            
            if not is_valid and self.fail_on_validation_error:
                error_msg = f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨: {validation_result.get('error_messages', [])}"
                logger.error(error_msg)
                raise AirflowException(error_msg)
            
            logger.info("ë°ì´í„° ê²€ì¦ ì™„ë£Œ", result=validation_result)
            return validation_result
            
        except Exception as e:
            logger.error("ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨", error=str(e))
            raise
    
    def _get_collection_task_id(self, context: Context) -> str:
        """ìˆ˜ì§‘ íƒœìŠ¤í¬ ID ì¶”ì •"""
        # ì¼ë°˜ì ì¸ ìˆ˜ì§‘ íƒœìŠ¤í¬ IDë“¤ì„ ì‹œë„
        possible_task_ids = [
            'collect_legal_data',
            'data_collection.collect_law_contents',
            'legal_data_collection'
        ]
        
        for task_id in possible_task_ids:
            try:
                result = context['task_instance'].xcom_pull(task_ids=task_id)
                if result:
                    return task_id
            except:
                continue
        
        raise AirflowException("ìˆ˜ì§‘ íƒœìŠ¤í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    def _perform_basic_validation(self, collection_result: Dict[str, Any]) -> Dict[str, Any]:
        """ê¸°ë³¸ ê²€ì¦ ìˆ˜í–‰"""
        validation_errors = []
        validation_warnings = []
        
        # ì„±ê³µ ì—¬ë¶€ í™•ì¸
        if not collection_result.get('success', False):
            validation_errors.append("ë°ì´í„° ìˆ˜ì§‘ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤")
        
        # í†µê³„ í™•ì¸
        stats = collection_result.get('stats', {})
        
        # ì˜¤ë¥˜ ê±´ìˆ˜ í™•ì¸
        error_count = stats.get('total_errors', 0)
        if error_count > 0:
            validation_warnings.append(f"ìˆ˜ì§‘ ì¤‘ {error_count}ê±´ì˜ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤")
        
        # ì²˜ë¦¬ëœ ë°ì´í„° í™•ì¸
        total_processed = stats.get('total_new_laws', 0) + stats.get('total_updated_laws', 0)
        if total_processed == 0:
            validation_warnings.append("ì²˜ë¦¬ëœ ë²•ë ¹ì´ ì—†ìŠµë‹ˆë‹¤")
        
        # ì²˜ë¦¬ ì‹œê°„ í™•ì¸
        processing_time = stats.get('total_processing_time', 0)
        if processing_time > 3600:  # 1ì‹œê°„ ì´ˆê³¼
            validation_warnings.append(f"ì²˜ë¦¬ ì‹œê°„ì´ ê¸¸ìŠµë‹ˆë‹¤: {processing_time:.1f}ì´ˆ")
        
        return {
            'is_valid': len(validation_errors) == 0,
            'error_messages': validation_errors,
            'warning_messages': validation_warnings,
            'validation_time': datetime.now().isoformat(),
            'validated_stats': stats
        }
    
    def _apply_validation_rules(self, collection_result: Dict[str, Any], 
                               rules: Dict[str, Any]) -> Dict[str, Any]:
        """ì¶”ê°€ ê²€ì¦ ê·œì¹™ ì ìš©"""
        additional_errors = []
        additional_warnings = []
        
        stats = collection_result.get('stats', {})
        
        # ìµœì†Œ ì²˜ë¦¬ ê±´ìˆ˜ í™•ì¸
        if 'min_processed_laws' in rules:
            min_laws = rules['min_processed_laws']
            total_processed = stats.get('total_new_laws', 0) + stats.get('total_updated_laws', 0)
            if total_processed < min_laws:
                additional_errors.append(f"ì²˜ë¦¬ëœ ë²•ë ¹ ìˆ˜ê°€ ìµœì†Œ ê¸°ì¤€ ë¯¸ë‹¬: {total_processed} < {min_laws}")
        
        # ìµœëŒ€ ì˜¤ë¥˜ ë¹„ìœ¨ í™•ì¸
        if 'max_error_rate' in rules:
            max_error_rate = rules['max_error_rate']
            total_found = stats.get('total_laws_found', 0)
            error_count = stats.get('total_errors', 0)
            
            if total_found > 0:
                error_rate = error_count / total_found
                if error_rate > max_error_rate:
                    additional_errors.append(f"ì˜¤ë¥˜ ë¹„ìœ¨ì´ í—ˆìš© ê¸°ì¤€ ì´ˆê³¼: {error_rate:.2%} > {max_error_rate:.2%}")
        
        # ìµœëŒ€ ì²˜ë¦¬ ì‹œê°„ í™•ì¸
        if 'max_processing_time' in rules:
            max_time = rules['max_processing_time']
            processing_time = stats.get('total_processing_time', 0)
            if processing_time > max_time:
                additional_warnings.append(f"ì²˜ë¦¬ ì‹œê°„ì´ ê¸°ì¤€ ì´ˆê³¼: {processing_time:.1f}ì´ˆ > {max_time}ì´ˆ")
        
        return {
            'additional_errors': additional_errors,
            'additional_warnings': additional_warnings,
            'rules_applied': list(rules.keys())
        }

class LegalDataNotificationOperator(BaseOperator):
    """
    ë²•ì œì²˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì•Œë¦¼ ì˜¤í¼ë ˆì´í„°
    """
    
    template_fields = ['message_template', 'notification_channels']
    ui_color = '#9C27B0'
    ui_fgcolor = '#FFFFFF'
    
    @apply_defaults
    def __init__(
        self,
        notification_type: str = 'completion',  # 'completion', 'error', 'warning'
        message_template: Optional[str] = None,
        notification_channels: List[str] = None,
        include_stats: bool = True,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.notification_type = notification_type
        self.message_template = message_template
        self.notification_channels = notification_channels or ['slack']
        self.include_stats = include_stats
    
    def execute(self, context: Context) -> Dict[str, Any]:
        """ì•Œë¦¼ ë°œì†¡ ì‹¤í–‰"""
        logger.info("ì•Œë¦¼ ë°œì†¡ ì‹œì‘", notification_type=self.notification_type)
        
        try:
            # ì´ì „ íƒœìŠ¤í¬ë“¤ì˜ ê²°ê³¼ ìˆ˜ì§‘
            task_results = self._collect_task_results(context)
            
            # ì•Œë¦¼ ë©”ì‹œì§€ ìƒì„±
            message = self._generate_message(task_results, context)
            
            # ì•Œë¦¼ ë°œì†¡
            notification_results = []
            
            for channel in self.notification_channels:
                try:
                    if channel == 'slack':
                        result = self._send_slack_notification(message, task_results)
                        notification_results.append({'channel': 'slack', 'success': True, 'result': result})
                    elif channel == 'email':
                        result = self._send_email_notification(message, task_results, context)
                        notification_results.append({'channel': 'email', 'success': True, 'result': result})
                    else:
                        logger.warning("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì•Œë¦¼ ì±„ë„", channel=channel)
                        
                except Exception as e:
                    logger.error("ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨", channel=channel, error=str(e))
                    notification_results.append({'channel': channel, 'success': False, 'error': str(e)})
            
            result = {
                'notification_type': self.notification_type,
                'channels_attempted': self.notification_channels,
                'results': notification_results,
                'message_sent': message,
                'send_time': datetime.now().isoformat()
            }
            
            logger.info("ì•Œë¦¼ ë°œì†¡ ì™„ë£Œ", result=result)
            return result
            
        except Exception as e:
            logger.error("ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨", error=str(e))
            raise
    
    def _collect_task_results(self, context: Context) -> Dict[str, Any]:
        """ì´ì „ íƒœìŠ¤í¬ ê²°ê³¼ ìˆ˜ì§‘"""
        results = {}
        
        # ì¼ë°˜ì ì¸ íƒœìŠ¤í¬ IDë“¤ì—ì„œ ê²°ê³¼ ìˆ˜ì§‘
        task_ids_to_check = [
            'generate_job_id',
            'check_prerequisites', 
            'collect_legal_data',
            'validate_collected_data',
            'update_sync_status'
        ]
        
        for task_id in task_ids_to_check:
            try:
                result = context['task_instance'].xcom_pull(task_ids=task_id)
                if result:
                    results[task_id] = result
            except:
                continue
        
        return results
    
    def _generate_message(self, task_results: Dict[str, Any], context: Context) -> str:
        """ì•Œë¦¼ ë©”ì‹œì§€ ìƒì„±"""
        if self.message_template:
            # í…œí”Œë¦¿ì´ ì œê³µëœ ê²½ìš° ì‚¬ìš©
            return self.message_template.format(**task_results, **context)
        
        # ê¸°ë³¸ ë©”ì‹œì§€ ìƒì„±
        job_id = task_results.get('generate_job_id', 'unknown')
        execution_date = context.get('execution_date', datetime.now())
        
        if self.notification_type == 'completion':
            return self._generate_completion_message(job_id, task_results, execution_date)
        elif self.notification_type == 'error':
            return self._generate_error_message(job_id, task_results, execution_date, context)
        elif self.notification_type == 'warning':
            return self._generate_warning_message(job_id, task_results, execution_date)
        else:
            return f"ë²•ì œì²˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì•Œë¦¼ - {self.notification_type}"
    
    def _generate_completion_message(self, job_id: str, task_results: Dict[str, Any], 
                                   execution_date: datetime) -> str:
        """ì™„ë£Œ ë©”ì‹œì§€ ìƒì„±"""
        collection_result = task_results.get('collect_legal_data', {})
        stats = collection_result.get('stats', {})
        
        message = f"""
âœ… ë²•ì œì²˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì™„ë£Œ

ğŸ†” ì‘ì—… ID: {job_id}
ğŸ“… ì‹¤í–‰ ì‹œê°„: {execution_date.strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:
â€¢ ì‹ ê·œ ë²•ë ¹: {stats.get('total_new_laws', 0)}ê°œ
â€¢ ì—…ë°ì´íŠ¸ ë²•ë ¹: {stats.get('total_updated_laws', 0)}ê°œ
â€¢ ì‹ ê·œ ì¡°í•­: {stats.get('total_new_articles', 0)}ê°œ
â€¢ ì—…ë°ì´íŠ¸ ì¡°í•­: {stats.get('total_updated_articles', 0)}ê°œ
â€¢ ì˜¤ë¥˜ ê±´ìˆ˜: {stats.get('total_errors', 0)}ê±´
â€¢ ì²˜ë¦¬ ì‹œê°„: {stats.get('total_processing_time', 0):.1f}ì´ˆ

ğŸ¯ ìƒíƒœ: {'ì„±ê³µ' if stats.get('total_errors', 0) == 0 else 'ë¶€ë¶„ ì„±ê³µ'}
        """.strip()
        
        return message
    
    def _generate_error_message(self, job_id: str, task_results: Dict[str, Any], 
                              execution_date: datetime, context: Context) -> str:
        """ì˜¤ë¥˜ ë©”ì‹œì§€ ìƒì„±"""
        failed_task = context.get('task_instance', {}).task_id if context.get('task_instance') else 'unknown'
        exception = context.get('exception', 'Unknown error')
        
        message = f"""
ğŸš¨ ë²•ì œì²˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨

ğŸ†” ì‘ì—… ID: {job_id}
ğŸ“… ì‹¤í–‰ ì‹œê°„: {execution_date.strftime('%Y-%m-%d %H:%M:%S')}
ğŸ“ ì‹¤íŒ¨ íƒœìŠ¤í¬: {failed_task}

âŒ ì˜¤ë¥˜ ë‚´ìš©:
{str(exception)}

ğŸ”§ ê´€ë¦¬ì í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.
        """.strip()
        
        return message
    
    def _generate_warning_message(self, job_id: str, task_results: Dict[str, Any], 
                                execution_date: datetime) -> str:
        """ê²½ê³  ë©”ì‹œì§€ ìƒì„±"""
        validation_result = task_results.get('validate_collected_data', {})
        warnings = validation_result.get('warning_messages', [])
        
        warning_text = '\n'.join([f"â€¢ {w}" for w in warnings[:5]])
        
        message = f"""
âš ï¸ ë²•ì œì²˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ê²½ê³ 

ğŸ†” ì‘ì—… ID: {job_id}
ğŸ“… ì‹¤í–‰ ì‹œê°„: {execution_date.strftime('%Y-%m-%d %H:%M:%S')}

ğŸ“‹ ê²½ê³  ì‚¬í•­:
{warning_text}

â„¹ï¸ íŒŒì´í”„ë¼ì¸ì€ ì™„ë£Œë˜ì—ˆìœ¼ë‚˜ ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.
        """.strip()
        
        return message
    
    def _send_slack_notification(self, message: str, task_results: Dict[str, Any]) -> Dict[str, Any]:
        """Slack ì•Œë¦¼ ë°œì†¡"""
        try:
            if self.notification_type == 'error':
                slack_service.send_error_alert(
                    title="ë²•ì œì²˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜",
                    message=message,
                    context=task_results
                )
            else:
                slack_service.send_info_message(
                    title="ë²•ì œì²˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì•Œë¦¼",
                    message=message
                )
            
            return {'status': 'sent', 'channel': 'slack'}
            
        except Exception as e:
            logger.error("Slack ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨", error=str(e))
            raise
    
    def _send_email_notification(self, message: str, task_results: Dict[str, Any], 
                               context: Context) -> Dict[str, Any]:
        """ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡"""
        try:
            from airflow.utils.email import send_email
            
            subject = f"ë²•ì œì²˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ - {self.notification_type.title()}"
            
            # ì´ë©”ì¼ ìˆ˜ì‹ ì ì„¤ì • (Airflow Variableì—ì„œ ì¡°íšŒ)
            from airflow.models import Variable
            recipients = Variable.get("legal_pipeline_email_recipients", 
                                    default_var="admin@company.com").split(',')
            
            send_email(
                to=recipients,
                subject=subject,
                html_content=f"<pre>{message}</pre>",
                mime_charset='utf-8'
            )
            
            return {'status': 'sent', 'channel': 'email', 'recipients': recipients}
            
        except Exception as e:
            logger.error("ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨", error=str(e))
            raise

class LegalDataCleanupOperator(BaseOperator):
    """
    ë²•ì œì²˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì •ë¦¬ ì˜¤í¼ë ˆì´í„°
    """
    
    template_fields = ['cleanup_days', 'cleanup_paths']
    ui_color = '#607D8B'
    ui_fgcolor = '#FFFFFF'
    
    @apply_defaults
    def __init__(
        self,
        cleanup_days: int = 7,           # ì •ë¦¬í•  íŒŒì¼ ë‚˜ì´ (ì¼)
        cleanup_paths: List[str] = None, # ì •ë¦¬í•  ê²½ë¡œ ëª©ë¡
        cleanup_logs: bool = True,       # ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì—¬ë¶€
        cleanup_temp: bool = True,       # ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì—¬ë¶€
        **kwargs
    ):
        super().__init__(**kwargs)
        self.cleanup_days = cleanup_days
        self.cleanup_paths = cleanup_paths or ['/tmp', '/var/log/airflow']
        self.cleanup_logs = cleanup_logs
        self.cleanup_temp = cleanup_temp
    
    def execute(self, context: Context) -> Dict[str, Any]:
        """ì •ë¦¬ ì‘ì—… ì‹¤í–‰"""
        logger.info("ì •ë¦¬ ì‘ì—… ì‹œì‘", cleanup_days=self.cleanup_days)
        
        cleanup_results = []
        
        try:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if self.cleanup_temp:
                temp_result = self._cleanup_temp_files()
                cleanup_results.append(temp_result)
            
            # ë¡œê·¸ íŒŒì¼ ì •ë¦¬
            if self.cleanup_logs:
                log_result = self._cleanup_log_files()
                cleanup_results.append(log_result)
            
            # ì¶”ê°€ ê²½ë¡œ ì •ë¦¬
            for path in self.cleanup_paths:
                path_result = self._cleanup_path(path)
                cleanup_results.append(path_result)
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬
            db_result = self._cleanup_database()
            cleanup_results.append(db_result)
            
            result = {
                'cleanup_completed': True,
                'cleanup_days': self.cleanup_days,
                'results': cleanup_results,
                'total_files_removed': sum(r.get('files_removed', 0) for r in cleanup_results),
                'total_space_freed_mb': sum(r.get('space_freed_mb', 0) for r in cleanup_results),
                'cleanup_time': datetime.now().isoformat()
            }
            
            logger.info("ì •ë¦¬ ì‘ì—… ì™„ë£Œ", result=result)
            return result
            
        except Exception as e:
            logger.error("ì •ë¦¬ ì‘ì—… ì‹¤íŒ¨", error=str(e))
            raise
    
    def _cleanup_temp_files(self) -> Dict[str, Any]:
        """ì„ì‹œ íŒŒì¼ ì •ë¦¬"""
        import os
        import glob
        from pathlib import Path
        
        logger.info("ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹œì‘")
        
        temp_patterns = [
            '/tmp/legal_pipeline_*',
            '/tmp/airflow_*',
            '/tmp/*.tmp'
        ]
        
        files_removed = 0
        space_freed = 0
        
        cutoff_time = datetime.now() - timedelta(days=self.cleanup_days)
        
        for pattern in temp_patterns:
            try:
                for file_path in glob.glob(pattern):
                    try:
                        file_stat = os.stat(file_path)
                        file_mtime = datetime.fromtimestamp(file_stat.st_mtime)
                        
                        if file_mtime < cutoff_time:
                            file_size = file_stat.st_size
                            os.remove(file_path)
                            files_removed += 1
                            space_freed += file_size
                            logger.debug("ì„ì‹œ íŒŒì¼ ì‚­ì œ", file_path=file_path)
                            
                    except Exception as e:
                        logger.warning("íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨", file_path=file_path, error=str(e))
                        
            except Exception as e:
                logger.warning("íŒ¨í„´ ì²˜ë¦¬ ì‹¤íŒ¨", pattern=pattern, error=str(e))
        
        result = {
            'type': 'temp_files',
            'files_removed': files_removed,
            'space_freed_mb': space_freed / (1024 * 1024),
            'patterns_checked': temp_patterns
        }
        
        logger.info("ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ", result=result)
        return result
    
    def _cleanup_log_files(self) -> Dict[str, Any]:
        """ë¡œê·¸ íŒŒì¼ ì •ë¦¬"""
        import os
        import glob
        
        logger.info("ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì‹œì‘")
        
        log_patterns = [
            '/var/log/airflow/*.log.*',
            '/var/log/legal-pipeline/*.log.*',
            '*.log.old'
        ]
        
        files_removed = 0
        space_freed = 0
        
        cutoff_time = datetime.now() - timedelta(days=self.cleanup_days)
        
        for pattern in log_patterns:
            try:
                for file_path in glob.glob(pattern):
                    try:
                        file_stat = os.stat(file_path)
                        file_mtime = datetime.fromtimestamp(file_stat.st_mtime)
                        
                        if file_mtime < cutoff_time:
                            file_size = file_stat.st_size
                            os.remove(file_path)
                            files_removed += 1
                            space_freed += file_size
                            logger.debug("ë¡œê·¸ íŒŒì¼ ì‚­ì œ", file_path=file_path)
                            
                    except Exception as e:
                        logger.warning("ë¡œê·¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨", file_path=file_path, error=str(e))
                        
            except Exception as e:
                logger.warning("ë¡œê·¸ íŒ¨í„´ ì²˜ë¦¬ ì‹¤íŒ¨", pattern=pattern, error=str(e))
        
        result = {
            'type': 'log_files',
            'files_removed': files_removed,
            'space_freed_mb': space_freed / (1024 * 1024),
            'patterns_checked': log_patterns
        }
        
        logger.info("ë¡œê·¸ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ", result=result)
        return result
    
    def _cleanup_path(self, path: str) -> Dict[str, Any]:
        """íŠ¹ì • ê²½ë¡œ ì •ë¦¬"""
        logger.info("ê²½ë¡œ ì •ë¦¬ ì‹œì‘", path=path)
        
        # ì•ˆì „ì„±ì„ ìœ„í•´ íŠ¹ì • ê²½ë¡œë§Œ í—ˆìš©
        allowed_paths = ['/tmp', '/var/log', '/var/cache']
        if not any(path.startswith(allowed) for allowed in allowed_paths):
            logger.warning("í—ˆìš©ë˜ì§€ ì•Šì€ ê²½ë¡œ", path=path)
            return {'type': 'path_cleanup', 'path': path, 'skipped': True, 'reason': 'not_allowed'}
        
        files_removed = 0
        space_freed = 0
        
        try:
            import os
            cutoff_time = datetime.now() - timedelta(days=self.cleanup_days)
            
            for root, dirs, files in os.walk(path):
                for file in files:
                    file_path = os.path.join(root, file)
                    try:
                        file_stat = os.stat(file_path)
                        file_mtime = datetime.fromtimestamp(file_stat.st_mtime)
                        
                        if file_mtime < cutoff_time:
                            file_size = file_stat.st_size
                            os.remove(file_path)
                            files_removed += 1
                            space_freed += file_size
                            
                    except Exception as e:
                        logger.warning("íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨", file_path=file_path, error=str(e))
                        
        except Exception as e:
            logger.error("ê²½ë¡œ ì •ë¦¬ ì‹¤íŒ¨", path=path, error=str(e))
        
        result = {
            'type': 'path_cleanup',
            'path': path,
            'files_removed': files_removed,
            'space_freed_mb': space_freed / (1024 * 1024)
        }
        
        logger.info("ê²½ë¡œ ì •ë¦¬ ì™„ë£Œ", result=result)
        return result
    
    def _cleanup_database(self) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬"""
        logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬ ì‹œì‘")
        
        try:
            repository = LegalDataRepository()
            
            # ì˜¤ë˜ëœ ë°°ì¹˜ ì‘ì—… ê¸°ë¡ ì •ë¦¬
            cutoff_date = datetime.now() - timedelta(days=self.cleanup_days * 2)  # ë°°ì¹˜ ê¸°ë¡ì€ ë” ì˜¤ë˜ ë³´ê´€
            
            with repository.transaction():
                # êµ¬í˜„ ì˜ˆì •: ì˜¤ë˜ëœ ë°°ì¹˜ ì‘ì—… ê¸°ë¡ ì‚­ì œ
                # repository.cleanup_old_batch_jobs(cutoff_date)
                pass
            
            result = {
                'type': 'database_cleanup',
                'cutoff_date': cutoff_date.isoformat(),
                'records_removed': 0,  # ì‹¤ì œ êµ¬í˜„ ì‹œ ì—…ë°ì´íŠ¸
                'tables_cleaned': ['batch_jobs']
            }
            
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬ ì™„ë£Œ", result=result)
            return result
            
        except Exception as e:
            logger.error("ë°ì´í„°ë² ì´ìŠ¤ ì •ë¦¬ ì‹¤íŒ¨", error=str(e))
            return {
                'type': 'database_cleanup',
                'error': str(e),
                'records_removed': 0
            }