"""
Kafka ê¸°ë°˜ ë²•ì œì²˜ API ë°ì´í„° íŒŒì´í”„ë¼ì¸ Airflow DAG
MOCK ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ ì™„ì „í•œ Producer/Consumer ì›Œí¬í”Œë¡œìš°
"""
from datetime import datetime, timedelta, date
from typing import Dict, Any, List
import asyncio
import uuid

from airflow import DAG
from airflow.operators.python import PythonOperator

from api.kafka_client import kafka_integrated_client
from streaming.producer import producer
from streaming.consumer import consumer
from streaming.models import EventType
from database.repository import LegalDataRepository
from mock.data_generator import MockDataGenerator
from notifications.slack_service import slack_service
from logging_config import get_logger

logger = get_logger(__name__)

# DAG ê¸°ë³¸ ì„¤ì •
DEFAULT_ARGS = {
    'owner': 'legal-data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=15),
    'execution_timeout': timedelta(hours=2),
    'sla': timedelta(hours=3),
}

# Kafka ê¸°ë°˜ DAG ì •ì˜
dag = DAG(
    'kafka_legal_data_pipeline_mock',
    default_args=DEFAULT_ARGS,
    description='Kafka ê¸°ë°˜ ë²•ì œì²˜ API ë°ì´í„° íŒŒì´í”„ë¼ì¸ - MOCK ë°ì´í„° í™œìš© ì™„ì „ ì›Œí¬í”Œë¡œìš°',
    schedule='0 2 * * *',  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ ì‹¤í–‰
    catchup=False,
    max_active_runs=1,
    tags=['legal', 'kafka', 'pipeline', 'mock', 'complete-workflow'],
    is_paused_upon_creation=False,
    doc_md="""
    # Kafka ê¸°ë°˜ ë²•ì œì²˜ API ë°ì´í„° íŒŒì´í”„ë¼ì¸ (MOCK ë°ì´í„°)
    
    ## ê°œìš”
    MOCK ë°ì´í„°ë¥¼ í™œìš©í•œ ì™„ì „í•œ Producer/Consumer ì›Œí¬í”Œë¡œìš°ì…ë‹ˆë‹¤.
    
    ## ì£¼ìš” íŠ¹ì§•
    - **MOCK ë°ì´í„°**: ê¸°ì¡´ MockDataGenerator í™œìš©ìœ¼ë¡œ ì‹¤ì œ API í˜¸ì¶œ ì—†ì´ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥
    - **ì™„ì „í•œ ì›Œí¬í”Œë¡œìš°**: Producer â†’ Consumer â†’ ì•Œë¦¼ê¹Œì§€ ì „ì²´ ê³¼ì •
    - **Kafka ìŠ¤íŠ¸ë¦¬ë°**: ì‹¤ì œ Kafka Producer/Consumer ì‚¬ìš©
    - **ë†’ì€ ì‹ ë¢°ì„±**: Kafka ë©”ì‹œì§€ ì˜ì†ì„±ìœ¼ë¡œ ë°ì´í„° ì†ì‹¤ ë°©ì§€
    - **í™•ì¥ì„±**: Producer/Consumer ë¶„ë¦¬ë¡œ ìˆ˜í‰ í™•ì¥ ê°€ëŠ¥
    - **ëª¨ë‹ˆí„°ë§**: ì‹¤ì‹œê°„ ì²˜ë¦¬ ìƒíƒœ ì¶”ì  ë° ìƒì„¸ í†µê³„
    
    ## ë°ì´í„° í”Œë¡œìš°
    1. **Health Check**: Kafka í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸
    2. **Producer**: MOCK ë²•ë ¹ ë°ì´í„° â†’ Kafka Topics
    3. **Consumer**: Kafka Topics â†’ MySQL Database ì €ì¥
    4. **Notification**: ì²˜ë¦¬ ê²°ê³¼ ì•Œë¦¼ ë°œì†¡
    """,

    params={
        'force_full_sync': False,
        'target_date': None,
        'batch_size': 30,  # MOCK ë°ì´í„°ìš©ìœ¼ë¡œ ì„¤ì •
        'notification_enabled': True,
        'kafka_enabled': True,
        'mock_laws_count': 25,  # ìƒì„±í•  MOCK ë²•ë ¹ ìˆ˜
        'consumer_timeout_seconds': 300,  # Consumer íƒ€ì„ì•„ì›ƒ
        'producer_failure_simulation': False  # Producer ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜
    }
)

# ==================== í—¬í¼ í•¨ìˆ˜ ====================

def run_async_task(async_func, *args, **kwargs):
    """ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸° í™˜ê²½ì—ì„œ ì‹¤í–‰"""
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        return loop.run_until_complete(async_func(*args, **kwargs))
    finally:
        loop.close()

# ==================== DAG íƒœìŠ¤í¬ í•¨ìˆ˜ë“¤ ====================

def check_kafka_health(**context) -> Dict[str, Any]:
    """Kafka í—¬ìŠ¤ ì²´í¬"""
    logger.info("Kafka í—¬ìŠ¤ ì²´í¬ ì‹œì‘")
    
    async def _check_kafka():
        """Kafka í´ëŸ¬ìŠ¤í„° ìƒíƒœ í™•ì¸"""
        async with kafka_integrated_client:
            health_result = await kafka_integrated_client.health_check()
            return health_result
    
    try:
        health_result = run_async_task(_check_kafka)
        
        if health_result.get('overall_status') == 'healthy':
            logger.info("Kafka í—¬ìŠ¤ ì²´í¬ ì„±ê³µ", health=health_result)
        else:
            logger.error("Kafka í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨", health=health_result)
            raise Exception(f"Kafka í´ëŸ¬ìŠ¤í„°ê°€ ë¹„ì •ìƒ ìƒíƒœì…ë‹ˆë‹¤: {health_result}")
        
        return health_result
        
    except Exception as e:
        logger.error("Kafka í—¬ìŠ¤ ì²´í¬ ì˜¤ë¥˜", error=str(e))
        raise

def kafka_produce_mock_data(**context) -> Dict[str, Any]:
    """Kafka Producerë¥¼ í†µí•œ MOCK ë°ì´í„° ì „ì†¡"""
    logger.info("Kafka Producer MOCK ë°ì´í„° ì „ì†¡ ì‹œì‘")
    
    async def _produce_mock_data(mock_count: int, failure_simulation: bool):
        """MOCK ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  Kafkaë¡œ ì „ì†¡"""
        
        # MOCK ë°ì´í„° ìƒì„±ê¸° ì´ˆê¸°í™”
        mock_generator = MockDataGenerator()
        
        # MOCK ë²•ë ¹ ë°ì´í„° ìƒì„±
        mock_documents = mock_generator.generate_multiple_documents(mock_count)
        logger.info(f"MOCK ë²•ë ¹ ë°ì´í„° {len(mock_documents)}ê°œ ìƒì„± ì™„ë£Œ")
        
        # Producer ì„¸ì…˜ ì‹œì‘
        async with producer.session():
            sent_count = 0
            failed_count = 0
            job_id = f"mock_producer_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            # ë°°ì¹˜ ì‹œì‘ ì´ë²¤íŠ¸ ì „ì†¡
            await producer.send_batch_status_event(
                job_id, "MOCK_PRODUCER", EventType.BATCH_STARTED
            )
            
            try:
                for i, doc in enumerate(mock_documents):
                    try:
                        # ì˜ë„ì  ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜
                        if failure_simulation and i % 10 == 0:  # 10%
                            raise Exception(f"ì˜ë„ì  ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜: {doc['id']}")
                        
                        # MOCK ë°ì´í„°ë¥¼ LawEvent í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        from streaming.models import LawEvent
                        from api.models import LawListItem
                        
                        # MockDataGenerator ì¶œë ¥ì„ LawListItemìœ¼ë¡œ ë³€í™˜
                        mock_law = LawListItem(
                            law_id=doc['id'],
                            law_master_no=doc['id'],  # MOCKì—ì„œëŠ” ë™ì¼í•˜ê²Œ ì‚¬ìš©
                            law_name=doc['title'],
                            enforcement_date=datetime.fromisoformat(doc['published_date'].replace('Z', '+00:00')).date(),
                            promulgation_date=datetime.fromisoformat(doc['published_date'].replace('Z', '+00:00')).date(),
                            law_type=doc['doc_type'],
                            ministry_name=doc['source'],
                            revision_type=doc['status']
                        )
                        
                        # LawEvent ìƒì„± ë° ì „ì†¡
                        success = await producer.send_law_event(mock_law)
                        
                        if success:
                            sent_count += 1
                            logger.debug(f"MOCK ë²•ë ¹ ì „ì†¡ ì„±ê³µ: {doc['id']}")
                        else:
                            failed_count += 1
                            logger.warning(f"MOCK ë²•ë ¹ ì „ì†¡ ì‹¤íŒ¨: {doc['id']}")
                        
                        # ì²˜ë¦¬ ì†ë„ ì‹œë®¬ë ˆì´ì…˜
                        await asyncio.sleep(0.1)
                        
                    except Exception as e:
                        failed_count += 1
                        logger.warning(f"MOCK ë²•ë ¹ ì²˜ë¦¬ ì‹¤íŒ¨: {doc.get('id', 'Unknown')} - {str(e)}")
                
                # ë°°ì¹˜ ì™„ë£Œ/ì‹¤íŒ¨ ì´ë²¤íŠ¸ ì „ì†¡
                if failed_count == 0:
                    await producer.send_batch_status_event(
                        job_id, "MOCK_PRODUCER", EventType.BATCH_COMPLETED,
                        processed_count=sent_count, error_count=0
                    )
                else:
                    await producer.send_batch_status_event(
                        job_id, "MOCK_PRODUCER", EventType.BATCH_FAILED,
                        processed_count=sent_count, error_count=failed_count
                    )
                
                return {
                    'sent_laws': sent_count,
                    'failed_laws': failed_count,
                    'total_laws': len(mock_documents),
                    'job_id': job_id
                }
                
            except Exception as e:
                # ì „ì²´ ë°°ì¹˜ ì‹¤íŒ¨
                await producer.send_batch_status_event(
                    job_id, "MOCK_PRODUCER", EventType.BATCH_FAILED,
                    processed_count=sent_count, error_count=failed_count + 1
                )
                raise
    
    try:
        # íŒŒë¼ë¯¸í„° í™•ì¸
        params = context.get('params', {})
        mock_count = params.get('mock_laws_count', 25)
        failure_simulation = params.get('producer_failure_simulation', False)
        
        # Producer ì‹¤í–‰
        start_time = datetime.now()
        result = run_async_task(_produce_mock_data, mock_count, failure_simulation)
        end_time = datetime.now()
        
        result['duration_seconds'] = (end_time - start_time).total_seconds()
        result['is_mock_data'] = True
        
        logger.info("Kafka Producer MOCK ë°ì´í„° ì „ì†¡ ì™„ë£Œ",
                   sent_laws=result.get('sent_laws', 0),
                   failed_laws=result.get('failed_laws', 0),
                   duration=result.get('duration_seconds', 0))
        
        # XComì— ê²°ê³¼ ì €ì¥ (Consumerì—ì„œ ì‚¬ìš©)
        context['task_instance'].xcom_push(
            key='producer_result',
            value=result
        )
        
        return result
        
    except Exception as e:
        logger.error("Kafka Producer MOCK ë°ì´í„° ì „ì†¡ ì‹¤íŒ¨", error=str(e))
        
        # ì˜¤ë¥˜ ì•Œë¦¼
        slack_service.send_error_alert(
            error=e,
            context={
                'job_name': 'Kafka Producer (MOCK)',
                'dag_id': context['dag'].dag_id,
                'task_id': context['task'].task_id,
                'logical_date': context['logical_date'].isoformat()
            }
        )
        
        raise

def kafka_consume_and_store(**context) -> Dict[str, Any]:
    """Kafka Consumerë¥¼ í†µí•œ ë°ì´í„° ìˆ˜ì‹  ë° DB ì €ì¥"""
    logger.info("Kafka Consumer ë°ì´í„° ìˆ˜ì‹  ë° ì €ì¥ ì‹œì‘")
    
    async def _consume_and_store_data(expected_count: int, timeout_seconds: int):
        """Kafkaì—ì„œ ë°ì´í„°ë¥¼ ì†Œë¹„í•˜ê³  DBì— ì €ì¥"""
        from streaming.config import Topics
        
        # Consumer ê·¸ë£¹ ID ìƒì„± (DAG ì‹¤í–‰ë³„ë¡œ ê³ ìœ )
        execution_date = context['logical_date'].strftime('%Y%m%d_%H%M%S')
        consumer_group_id = f"dag-consumer-{execution_date}"
        
        # Consumer ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        consumer_instance = consumer.__class__(group_id=consumer_group_id)
        
        consumed_count = 0
        stored_count = 0
        failed_count = 0
        
        try:
            # Consumer ì„¸ì…˜ ì‹œì‘ (ë²•ë ¹ ì´ë²¤íŠ¸ë§Œ êµ¬ë…)
            async with consumer_instance.session([Topics.LAW_EVENTS]):
                logger.info(f"Consumer ì‹œì‘ - ì˜ˆìƒ ë©”ì‹œì§€: {expected_count}, íƒ€ì„ì•„ì›ƒ: {timeout_seconds}ì´ˆ")
                
                start_time = datetime.now()
                
                # ë©”ì‹œì§€ ì†Œë¹„ (íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬)
                while consumed_count < expected_count:
                    current_time = datetime.now()
                    elapsed = (current_time - start_time).total_seconds()
                    
                    if elapsed > timeout_seconds:
                        logger.warning(f"Consumer íƒ€ì„ì•„ì›ƒ - ì²˜ë¦¬ëœ ë©”ì‹œì§€: {consumed_count}/{expected_count}")
                        break
                    
                    # ë‹¨ì¼ ë©”ì‹œì§€ ì†Œë¹„ (ë…¼ë¸”ë¡œí‚¹)
                    try:
                        # Consumer pollì„ ì§ì ‘ í˜¸ì¶œí•˜ì—¬ ë‹¨ì¼ ë©”ì‹œì§€ ì²˜ë¦¬
                        message_batch = consumer_instance.consumer.poll(timeout_ms=1000)
                        
                        if not message_batch:
                            continue
                        
                        # ë°°ì¹˜ì˜ ê° ë©”ì‹œì§€ ì²˜ë¦¬
                        for topic_partition, messages in message_batch.items():
                            for message in messages:
                                try:
                                    await consumer_instance._process_message(message)
                                    consumed_count += 1
                                    stored_count += 1
                                    
                                    logger.debug(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì™„ë£Œ: {consumed_count}/{expected_count}")
                                    
                                    # ì»¤ë°‹
                                    consumer_instance.consumer.commit()
                                    
                                except Exception as e:
                                    consumed_count += 1
                                    failed_count += 1
                                    logger.warning(f"ë©”ì‹œì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    
                    except Exception as e:
                        logger.warning(f"ë©”ì‹œì§€ polling ì˜¤ë¥˜: {str(e)}")
                        await asyncio.sleep(1)
                
                return {
                    'consumed_messages': consumed_count,
                    'stored_laws': stored_count,
                    'failed_stores': failed_count,
                    'expected_count': expected_count,
                    'timeout_reached': consumed_count < expected_count
                }
                
        except Exception as e:
            logger.error("Consumer ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜", error=str(e))
            return {
                'consumed_messages': consumed_count,
                'stored_laws': stored_count,
                'failed_stores': failed_count + 1,
                'expected_count': expected_count,
                'error': str(e)
            }
    
    try:
        # Producer ê²°ê³¼ì—ì„œ ì˜ˆìƒ ë©”ì‹œì§€ ìˆ˜ ê°€ì ¸ì˜¤ê¸°
        producer_result = context['task_instance'].xcom_pull(
            task_ids='kafka_produce_mock_data',
            key='producer_result'
        )
        
        if not producer_result:
            raise ValueError("Producer ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        expected_count = producer_result.get('sent_laws', 0)
        
        if expected_count == 0:
            logger.warning("ì²˜ë¦¬í•  ë©”ì‹œì§€ê°€ ì—†ìŠµë‹ˆë‹¤")
            return {
                'consumed_messages': 0,
                'stored_laws': 0,
                'failed_stores': 0
            }
        
        # íŒŒë¼ë¯¸í„° í™•ì¸
        params = context.get('params', {})
        timeout_seconds = params.get('consumer_timeout_seconds', 300)
        
        # Consumer ì‹¤í–‰
        start_time = datetime.now()
        result = run_async_task(_consume_and_store_data, expected_count, timeout_seconds)
        end_time = datetime.now()
        
        result['duration_seconds'] = (end_time - start_time).total_seconds()
        result['expected_messages'] = expected_count
        
        logger.info("Kafka Consumer ë°ì´í„° ìˆ˜ì‹  ë° ì €ì¥ ì™„ë£Œ",
                   consumed_messages=result.get('consumed_messages', 0),
                   stored_laws=result.get('stored_laws', 0),
                   failed_stores=result.get('failed_stores', 0),
                   duration=result.get('duration_seconds', 0))
        
        return result
        
    except Exception as e:
        logger.error("Kafka Consumer ë°ì´í„° ìˆ˜ì‹  ë° ì €ì¥ ì‹¤íŒ¨", error=str(e))
        
        # ì˜¤ë¥˜ ì•Œë¦¼
        slack_service.send_error_alert(
            error=e,
            context={
                'job_name': 'Kafka Consumer',
                'dag_id': context['dag'].dag_id,
                'task_id': context['task'].task_id,
                'logical_date': context['logical_date'].isoformat()
            }
        )
        
        raise

def send_completion_notification(**context) -> None:
    """ì™„ë£Œ ì•Œë¦¼ ë°œì†¡"""
    logger.info("ì™„ë£Œ ì•Œë¦¼ ë°œì†¡ ì‹œì‘")
    
    try:
        # íŒŒë¼ë¯¸í„° í™•ì¸
        params = context.get('params', {})
        if not params.get('notification_enabled', True):
            logger.info("ì•Œë¦¼ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
            return
        
        # Producer ë° Consumer ê²°ê³¼ ìˆ˜ì§‘
        producer_result = context['task_instance'].xcom_pull(
            task_ids='kafka_produce_mock_data'
        )
        consumer_result = context['task_instance'].xcom_pull(
            task_ids='kafka_consume_and_store'
        )
        
        if not producer_result or not consumer_result:
            logger.warning("Producer ë˜ëŠ” Consumer ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # í†µê³„ ì •ë³´ ìƒì„±
        produced_laws = producer_result.get('sent_laws', 0)
        producer_failures = producer_result.get('failed_laws', 0)
        consumed_messages = consumer_result.get('consumed_messages', 0)
        stored_laws = consumer_result.get('stored_laws', 0)
        consumer_failures = consumer_result.get('failed_stores', 0)
        timeout_reached = consumer_result.get('timeout_reached', False)
        
        producer_duration = producer_result.get('duration_seconds', 0)
        consumer_duration = consumer_result.get('duration_seconds', 0)
        total_duration = producer_duration + consumer_duration
        
        # ì„±ê³µ/ì‹¤íŒ¨ ê²°ì •
        total_failures = producer_failures + consumer_failures
        is_success = total_failures == 0 and not timeout_reached
        
        # BatchResult ìƒì„±
        from notifications.slack_service import BatchResult
        
        # ìƒíƒœ ì•„ì´ì½˜ ë° ë©”ì‹œì§€
        if is_success:
            status_icon = "âœ…"
            status_text = "ì„±ê³µ"
        elif timeout_reached:
            status_icon = "â°"
            status_text = "ë¶€ë¶„ ì„±ê³µ (íƒ€ì„ì•„ì›ƒ)"
        else:
            status_icon = "âš ï¸"
            status_text = "ë¶€ë¶„ ì„±ê³µ"
        
        # ìƒì„¸ ë©”ì‹œì§€ ìƒì„±
        detail_message = f"""
{status_icon} **Kafka íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ ê²°ê³¼**

ğŸ“Š **Producer í†µê³„**
â€¢ ìƒì„±ëœ MOCK ë°ì´í„°: {producer_result.get('total_laws', 0)}ê°œ
â€¢ Kafka ì „ì†¡ ì„±ê³µ: {produced_laws}ê°œ
â€¢ Kafka ì „ì†¡ ì‹¤íŒ¨: {producer_failures}ê°œ
â€¢ Producer ì²˜ë¦¬ ì‹œê°„: {producer_duration:.1f}ì´ˆ

ğŸ“¥ **Consumer í†µê³„**  
â€¢ Kafka ë©”ì‹œì§€ ìˆ˜ì‹ : {consumed_messages}ê°œ
â€¢ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥: {stored_laws}ê°œ
â€¢ ì €ì¥ ì‹¤íŒ¨: {consumer_failures}ê°œ
â€¢ Consumer ì²˜ë¦¬ ì‹œê°„: {consumer_duration:.1f}ì´ˆ

ğŸ” **ì „ì²´ í†µê³„**
â€¢ ì´ ì²˜ë¦¬ ì‹œê°„: {total_duration:.1f}ì´ˆ
â€¢ ì „ì²´ ì„±ê³µë¥ : {((stored_laws / produced_laws) * 100):.1f}%
â€¢ íŒŒì´í”„ë¼ì¸ íƒ€ì…: MOCK ë°ì´í„° í…ŒìŠ¤íŠ¸
        """.strip()
        
        if timeout_reached:
            detail_message += f"\nâš ï¸ Consumerê°€ {params.get('consumer_timeout_seconds', 300)}ì´ˆ íƒ€ì„ì•„ì›ƒì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤."
        
        batch_result = BatchResult(
            job_name="Kafka ê¸°ë°˜ ë²•ì œì²˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ (MOCK)",
            success=is_success,
            processed_laws=stored_laws,
            processed_articles=0,
            error_count=total_failures,
            error_message=None if is_success else f"ì´ {total_failures}ê°œ ì˜¤ë¥˜ ë°œìƒ" + (" (íƒ€ì„ì•„ì›ƒ í¬í•¨)" if timeout_reached else ""),
            duration=f"{total_duration:.1f}ì´ˆ",
            detail_message=detail_message
        )
        
        # ìŠ¬ë™ ì•Œë¦¼ ë°œì†¡
        success = slack_service.send_batch_completion_notice(batch_result)
        
        if success:
            logger.info("ì™„ë£Œ ì•Œë¦¼ ë°œì†¡ ì„±ê³µ")
        else:
            logger.warning("ì™„ë£Œ ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨")
        
        logger.info("ì™„ë£Œ ì•Œë¦¼ ë°œì†¡ ì™„ë£Œ")
        
    except Exception as e:
        logger.error("ì™„ë£Œ ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨", error=str(e))

# ==================== DAG íƒœìŠ¤í¬ ì •ì˜ ====================

# Kafka í—¬ìŠ¤ ì²´í¬
kafka_health_check_task = PythonOperator(
    task_id='check_kafka_health',
    python_callable=check_kafka_health,
    dag=dag,
    execution_timeout=timedelta(minutes=5),
    doc_md="Kafka í´ëŸ¬ìŠ¤í„°ì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
)

# Kafka Producer - MOCK ë°ì´í„° ì „ì†¡
kafka_produce_task = PythonOperator(
    task_id='kafka_produce_mock_data',
    python_callable=kafka_produce_mock_data,
    dag=dag,
    execution_timeout=timedelta(minutes=30),
    doc_md="""
    MOCK ë²•ë ¹ ë°ì´í„°ë¥¼ MockDataGeneratorë¡œ ìƒì„±í•˜ê³  Kafka í† í”½ìœ¼ë¡œ ì „ì†¡í•©ë‹ˆë‹¤.
    - MockDataGeneratorë¥¼ ì‚¬ìš©í•˜ì—¬ í˜„ì‹¤ì ì¸ ë²•ë ¹ ë°ì´í„° ìƒì„±
    - Kafka Producerë¥¼ í†µí•´ law_events í† í”½ìœ¼ë¡œ ì „ì†¡
    - ì˜ë„ì  ì‹¤íŒ¨ ì‹œë®¬ë ˆì´ì…˜ ì§€ì›
    """
)

# Kafka Consumer - ë°ì´í„° ìˆ˜ì‹  ë° ì €ì¥
kafka_consume_task = PythonOperator(
    task_id='kafka_consume_and_store',
    python_callable=kafka_consume_and_store,
    dag=dag,
    execution_timeout=timedelta(minutes=45),
    doc_md="""
    Kafka í† í”½ì—ì„œ ë²•ë ¹ ë°ì´í„°ë¥¼ ìˆ˜ì‹ í•˜ê³  ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.
    - law_events í† í”½ì„ êµ¬ë…í•˜ì—¬ ë©”ì‹œì§€ ì†Œë¹„
    - ìˆ˜ì‹ ëœ ë°ì´í„°ë¥¼ MySQL ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥  
    - íƒ€ì„ì•„ì›ƒ ì²˜ë¦¬ë¡œ ë¬´í•œ ëŒ€ê¸° ë°©ì§€
    - DAG ì‹¤í–‰ë³„ ê³ ìœ  Consumer Group ì‚¬ìš©
    """
)

# ì™„ë£Œ ì•Œë¦¼
completion_notification_task = PythonOperator(
    task_id='send_completion_notification',
    python_callable=send_completion_notification,
    dag=dag,
    doc_md="""
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ì•Œë¦¼ì„ ë°œì†¡í•©ë‹ˆë‹¤.
    - Producerì™€ Consumer ì‹¤í–‰ í†µê³„ ì¢…í•©
    - ì„±ê³µ/ì‹¤íŒ¨ ìƒíƒœ ë° ìƒì„¸ ë©”íŠ¸ë¦­ í¬í•¨
    - Slackì„ í†µí•œ ì•Œë¦¼ ë°œì†¡
    """
)

# ==================== íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì • ====================

kafka_health_check_task >> kafka_produce_task >> kafka_consume_task >> completion_notification_task
