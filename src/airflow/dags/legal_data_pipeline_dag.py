"""
ë²•ì œì²˜ API ë°ì´í„° íŒŒì´í”„ë¼ì¸ Airflow DAG
ì¦ë¶„ ì—…ë°ì´íŠ¸ ë°°ì¹˜ ì‘ì—… êµ¬í˜„ - Task 8

Requirements: 5.1, 5.2, 5.3, 5.4, 9.2
"""
from datetime import datetime, timedelta, date
from typing import Dict, Any, List
import uuid
import json

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.bash import BashOperator
from airflow.operators.dummy import DummyOperator
from airflow.sensors.filesystem import FileSensor
from airflow.utils.dates import days_ago
from airflow.utils.task_group import TaskGroup
from airflow.models import Variable
from airflow.exceptions import AirflowException, AirflowSkipException
from airflow.utils.email import send_email
from airflow.hooks.base import BaseHook

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ ì„í¬íŠ¸
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '../../..'))

from src.api.client import api_client
# from src.processors.incremental_processor import incremental_processor  # ì‚­ì œë¨
from src.database.repository import LegalDataRepository
from src.notifications.slack_service import slack_service
from src.logging_config import get_logger

# ì»¤ìŠ¤í…€ ì˜¤í¼ë ˆì´í„° ì„í¬íŠ¸
from ..operators.legal_data_operators import (
    LegalAPIHealthCheckOperator,
    LegalDataCollectionOperator,
    LegalDataValidationOperator,
    LegalDataNotificationOperator,
    LegalDataCleanupOperator
)

logger = get_logger(__name__)

# DAG ê¸°ë³¸ ì„¤ì •
DEFAULT_ARGS = {
    'owner': 'legal-data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'retry_exponential_backoff': True,
    'max_retry_delay': timedelta(minutes=30),
    'execution_timeout': timedelta(hours=2),
    'sla': timedelta(hours=3),
}

# DAG ì •ì˜
dag = DAG(
    'legal_data_pipeline',
    default_args=DEFAULT_ARGS,
    description='ë²•ì œì²˜ API ë°ì´í„° íŒŒì´í”„ë¼ì¸ - ì¦ë¶„ ì—…ë°ì´íŠ¸',
    schedule_interval='0 2 * * *',  # ë§¤ì¼ ìƒˆë²½ 2ì‹œ ì‹¤í–‰
    catchup=False,
    max_active_runs=1,
    tags=['legal', 'data-pipeline', 'incremental'],
    doc_md="""
    # ë²•ì œì²˜ API ë°ì´í„° íŒŒì´í”„ë¼ì¸
    
    ## ê°œìš”
    ë²•ì œì²˜ì—ì„œ ì œê³µí•˜ëŠ” APIë¥¼ í†µí•´ ë²•ë ¹ ë° ì¡°í•­ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  
    MySQL ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ì¦ë¶„ ì—…ë°ì´íŠ¸ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤.
    
    ## ì£¼ìš” ê¸°ëŠ¥
    - ë§ˆì§€ë§‰ ë™ê¸°í™” ë‚ ì§œ ê¸°ì¤€ ì¦ë¶„ ì—…ë°ì´íŠ¸
    - API ìƒíƒœ ëª¨ë‹ˆí„°ë§ ë° í—¬ìŠ¤ì²´í¬
    - ë°ì´í„° ê²€ì¦ ë° ì •í•©ì„± í™•ì¸
    - ì˜¤ë¥˜ ë°œìƒ ì‹œ ìë™ ì¬ì‹œë„ ë° ì•Œë¦¼
    - ë°°ì¹˜ ì‘ì—… ìƒíƒœ ì¶”ì  ë° ëª¨ë‹ˆí„°ë§
    
    ## ì‹¤í–‰ ì£¼ê¸°
    ë§¤ì¼ ìƒˆë²½ 2ì‹œì— ìë™ ì‹¤í–‰ë˜ë©°, ìˆ˜ë™ ì‹¤í–‰ë„ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """,
    params={
        'force_full_sync': False,
        'target_date': None,
        'skip_validation': False,
        'notification_enabled': True
    }
)

# ==================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ====================

def generate_job_id(**context) -> str:
    """ë°°ì¹˜ ì‘ì—… ID ìƒì„±"""
    execution_date = context['execution_date']
    job_id = f"legal_pipeline_{execution_date.strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
    
    # XComì— ì €ì¥í•˜ì—¬ ë‹¤ë¥¸ íƒœìŠ¤í¬ì—ì„œ ì‚¬ìš©
    context['task_instance'].xcom_push(key='job_id', value=job_id)
    
    logger.info("ë°°ì¹˜ ì‘ì—… ID ìƒì„±", job_id=job_id, execution_date=execution_date)
    return job_id

def get_job_id(**context) -> str:
    """XComì—ì„œ ì‘ì—… ID ì¡°íšŒ"""
    job_id = context['task_instance'].xcom_pull(key='job_id', task_ids='generate_job_id')
    if not job_id:
        raise AirflowException("ì‘ì—… IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    return job_id

def send_notification(title: str, message: str, is_error: bool = False, **context):
    """ì•Œë¦¼ ë°œì†¡"""
    try:
        if context.get('params', {}).get('notification_enabled', True):
            if is_error:
                slack_service.send_error_alert(title=title, message=message, context=context)
            else:
                slack_service.send_info_message(title=title, message=message)
        
        logger.info("ì•Œë¦¼ ë°œì†¡ ì™„ë£Œ", title=title, is_error=is_error)
        
    except Exception as e:
        logger.error("ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨", error=str(e), title=title)

# ==================== íƒœìŠ¤í¬ í•¨ìˆ˜ ====================

def check_prerequisites(**context) -> Dict[str, Any]:
    """ì‚¬ì „ ì¡°ê±´ í™•ì¸"""
    logger.info("ì‚¬ì „ ì¡°ê±´ í™•ì¸ ì‹œì‘")
    
    try:
        repository = LegalDataRepository()
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
        with repository.transaction():
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸ ì™„ë£Œ")
        
        # API ìƒíƒœ í™•ì¸
        health_status = api_client.health_check()
        if not health_status.is_healthy:
            raise AirflowException(f"API ìƒíƒœ ë¶ˆëŸ‰: {health_status.error_message}")
        
        # ë§ˆì§€ë§‰ ë™ê¸°í™” ë‚ ì§œ í™•ì¸ (repositoryì—ì„œ ì§ì ‘ ì¡°íšŒ)
        repository = LegalDataRepository()
        last_sync_date = repository.get_last_sync_date("INCREMENTAL") or date.today() - timedelta(days=7)
        
        result = {
            'database_healthy': True,
            'api_healthy': health_status.is_healthy,
            'api_response_time': health_status.response_time_ms,
            'last_sync_date': last_sync_date.isoformat(),
            'check_time': datetime.now().isoformat()
        }
        
        logger.info("ì‚¬ì „ ì¡°ê±´ í™•ì¸ ì™„ë£Œ", result=result)
        return result
        
    except Exception as e:
        logger.error("ì‚¬ì „ ì¡°ê±´ í™•ì¸ ì‹¤íŒ¨", error=str(e))
        send_notification(
            title="ë²•ì œì²˜ íŒŒì´í”„ë¼ì¸ ì‚¬ì „ ì¡°ê±´ í™•ì¸ ì‹¤íŒ¨",
            message=f"ì˜¤ë¥˜: {str(e)}",
            is_error=True,
            **context
        )
        raise

def determine_update_strategy(**context) -> Dict[str, Any]:
    """ì—…ë°ì´íŠ¸ ì „ëµ ê²°ì •"""
    logger.info("ì—…ë°ì´íŠ¸ ì „ëµ ê²°ì • ì‹œì‘")
    
    try:
        # íŒŒë¼ë¯¸í„° í™•ì¸
        force_full_sync = context.get('params', {}).get('force_full_sync', False)
        target_date = context.get('params', {}).get('target_date')
        
        # ì²˜ë¦¬í•  ë‚ ì§œ ëª©ë¡ ì¡°íšŒ
        if target_date:
            dates_to_process = [datetime.strptime(target_date, '%Y-%m-%d').date()]
            strategy = 'target_date'
        elif force_full_sync:
            # ì „ì²´ ë™ê¸°í™”ëŠ” ë³„ë„ DAGì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ìŠ¤í‚µ
            raise AirflowSkipException("ì „ì²´ ë™ê¸°í™”ëŠ” ë³„ë„ DAGì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤")
        else:
            # ì²˜ë¦¬í•  ë‚ ì§œ ëª©ë¡ ìƒì„± (repositoryì—ì„œ ì§ì ‘ ê³„ì‚°)
            repository = LegalDataRepository()
            last_processed = repository.get_last_sync_date("INCREMENTAL") or date.today() - timedelta(days=7)
            dates_to_process = []
            current_date = last_processed + timedelta(days=1)
            while current_date <= date.today():
                dates_to_process.append(current_date)
                current_date += timedelta(days=1)
            strategy = 'incremental'
        
        if not dates_to_process:
            logger.info("ì²˜ë¦¬í•  ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤")
            raise AirflowSkipException("ì²˜ë¦¬í•  ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤")
        
        result = {
            'strategy': strategy,
            'dates_to_process': [d.isoformat() for d in dates_to_process],
            'total_dates': len(dates_to_process),
            'date_range': {
                'start': dates_to_process[0].isoformat(),
                'end': dates_to_process[-1].isoformat()
            }
        }
        
        logger.info("ì—…ë°ì´íŠ¸ ì „ëµ ê²°ì • ì™„ë£Œ", result=result)
        
        # XComì— ì €ì¥
        context['task_instance'].xcom_push(key='update_strategy', value=result)
        
        return result
        
    except AirflowSkipException:
        raise
    except Exception as e:
        logger.error("ì—…ë°ì´íŠ¸ ì „ëµ ê²°ì • ì‹¤íŒ¨", error=str(e))
        raise

def collect_updated_laws(**context) -> Dict[str, Any]:
    """ì—…ë°ì´íŠ¸ëœ ë²•ë ¹ ëª©ë¡ ìˆ˜ì§‘"""
    logger.info("ì—…ë°ì´íŠ¸ëœ ë²•ë ¹ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘")
    
    try:
        job_id = get_job_id(**context)
        strategy_info = context['task_instance'].xcom_pull(
            key='update_strategy', 
            task_ids='determine_update_strategy'
        )
        
        if not strategy_info:
            raise AirflowException("ì—…ë°ì´íŠ¸ ì „ëµ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        dates_to_process = [
            datetime.strptime(d, '%Y-%m-%d').date() 
            for d in strategy_info['dates_to_process']
        ]
        
        collected_laws = []
        total_laws = 0
        
        # ê° ë‚ ì§œë³„ë¡œ ë²•ë ¹ ëª©ë¡ ìˆ˜ì§‘
        for target_date in dates_to_process:
            try:
                # APIì—ì„œ ë²•ë ¹ ëª©ë¡ ì¡°íšŒ (ë‚ ì§œ í•„í„°ë§ì€ í´ë¼ì´ì–¸íŠ¸ì—ì„œ)
                all_laws = api_client.collect_law_list()
                laws_on_date = [law for law in all_laws if hasattr(law, 'promulgation_date') and law.promulgation_date == target_date]
                collected_laws.extend([
                    {
                        'law_id': law.law_id,
                        'law_master_no': law.law_master_no,
                        'law_name': law.law_name,
                        'enforcement_date': law.enforcement_date.isoformat(),
                        'promulgation_date': law.promulgation_date.isoformat() if law.promulgation_date else None,
                        'target_date': target_date.isoformat()
                    }
                    for law in laws_on_date
                ])
                total_laws += len(laws_on_date)
                
                logger.info("ë‚ ì§œë³„ ë²•ë ¹ ìˆ˜ì§‘ ì™„ë£Œ", 
                           target_date=target_date, 
                           count=len(laws_on_date))
                
            except Exception as e:
                logger.error("ë‚ ì§œë³„ ë²•ë ¹ ìˆ˜ì§‘ ì‹¤íŒ¨", 
                           target_date=target_date, 
                           error=str(e))
                # ê°œë³„ ë‚ ì§œ ì‹¤íŒ¨ëŠ” ì „ì²´ ì‹¤íŒ¨ë¡œ ì´ì–´ì§€ì§€ ì•Šë„ë¡ í•¨
                continue
        
        result = {
            'job_id': job_id,
            'collected_laws': collected_laws,
            'total_laws': total_laws,
            'processed_dates': len(dates_to_process),
            'collection_time': datetime.now().isoformat()
        }
        
        logger.info("ë²•ë ¹ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ", 
                   total_laws=total_laws, 
                   processed_dates=len(dates_to_process))
        
        # XComì— ì €ì¥ (í¬ê¸° ì œí•œ ê³ ë ¤)
        if len(collected_laws) > 1000:
            # í° ë°ì´í„°ëŠ” ìš”ì•½ ì •ë³´ë§Œ ì €ì¥
            summary_result = {
                'job_id': job_id,
                'total_laws': total_laws,
                'processed_dates': len(dates_to_process),
                'collection_time': datetime.now().isoformat(),
                'data_truncated': True
            }
            context['task_instance'].xcom_push(key='collected_laws', value=summary_result)
        else:
            context['task_instance'].xcom_push(key='collected_laws', value=result)
        
        return result
        
    except Exception as e:
        logger.error("ë²•ë ¹ ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨", error=str(e))
        send_notification(
            title="ë²•ë ¹ ëª©ë¡ ìˆ˜ì§‘ ì‹¤íŒ¨",
            message=f"ì‘ì—… ID: {get_job_id(**context)}\nì˜¤ë¥˜: {str(e)}",
            is_error=True,
            **context
        )
        raise

def collect_law_contents(**context) -> Dict[str, Any]:
    """ë²•ë ¹ ë³¸ë¬¸ ìˆ˜ì§‘"""
    logger.info("ë²•ë ¹ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘")
    
    try:
        job_id = get_job_id(**context)
        collected_laws_info = context['task_instance'].xcom_pull(
            key='collected_laws',
            task_ids='collect_updated_laws'
        )
        
        if not collected_laws_info or collected_laws_info.get('data_truncated'):
            # í° ë°ì´í„°ì˜ ê²½ìš° ë‹¤ì‹œ ì¡°íšŒ
            strategy_info = context['task_instance'].xcom_pull(
                key='update_strategy',
                task_ids='determine_update_strategy'
            )
            dates_to_process = [
                datetime.strptime(d, '%Y-%m-%d').date()
                for d in strategy_info['dates_to_process']
            ]
            
            # Mock í†µê³„ ìƒì„± (ì‹¤ì œ ì²˜ë¦¬ ë¡œì§ì€ ë³„ë„ êµ¬í˜„ í•„ìš”)
            all_stats = [{
                'target_date': date.today(),
                'total_laws_found': len(collected_laws),
                'new_laws': 0,
                'updated_laws': 0,
                'error_count': 0
            }]
            
            result = {
                'job_id': job_id,
                'processing_completed': True,
                'daily_stats': [
                    {
                        'target_date': stats.target_date.isoformat(),
                        'total_laws_found': stats.total_laws_found,
                        'new_laws': stats.new_laws,
                        'updated_laws': stats.updated_laws,
                        'new_articles': stats.new_articles,
                        'updated_articles': stats.updated_articles,
                        'error_count': stats.error_count,
                        'processing_time_seconds': stats.processing_time_seconds
                    }
                    for stats in all_stats
                ],
                'total_stats': {
                    'total_new_laws': sum(s.new_laws for s in all_stats),
                    'total_updated_laws': sum(s.updated_laws for s in all_stats),
                    'total_new_articles': sum(s.new_articles for s in all_stats),
                    'total_updated_articles': sum(s.updated_articles for s in all_stats),
                    'total_errors': sum(s.error_count for s in all_stats),
                    'total_processing_time': sum(s.processing_time_seconds for s in all_stats)
                }
            }
            
        else:
            # ì‘ì€ ë°ì´í„°ì˜ ê²½ìš° ê°œë³„ ì²˜ë¦¬
            collected_laws = collected_laws_info.get('collected_laws', [])
            
            processed_count = 0
            error_count = 0
            
            for law_info in collected_laws:
                try:
                    # ë²•ë ¹ ë³¸ë¬¸ ìˆ˜ì§‘
                    law_content = api_client.collect_law_content(law_info['law_id'])
                    
                    # ì¡°í•­ ìˆ˜ì§‘
                    articles = api_client.collect_law_articles(law_info['law_master_no'])
                    
                    processed_count += 1
                    
                except Exception as e:
                    logger.error("ê°œë³„ ë²•ë ¹ ì²˜ë¦¬ ì‹¤íŒ¨",
                               law_id=law_info['law_id'],
                               error=str(e))
                    error_count += 1
            
            result = {
                'job_id': job_id,
                'processed_laws': processed_count,
                'error_count': error_count,
                'processing_completed': True
            }
        
        logger.info("ë²•ë ¹ ë³¸ë¬¸ ìˆ˜ì§‘ ì™„ë£Œ", result=result)
        
        # XComì— ì €ì¥
        context['task_instance'].xcom_push(key='content_collection_result', value=result)
        
        return result
        
    except Exception as e:
        logger.error("ë²•ë ¹ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨", error=str(e))
        send_notification(
            title="ë²•ë ¹ ë³¸ë¬¸ ìˆ˜ì§‘ ì‹¤íŒ¨",
            message=f"ì‘ì—… ID: {get_job_id(**context)}\nì˜¤ë¥˜: {str(e)}",
            is_error=True,
            **context
        )
        raise

def validate_collected_data(**context) -> Dict[str, Any]:
    """ìˆ˜ì§‘ëœ ë°ì´í„° ê²€ì¦"""
    logger.info("ë°ì´í„° ê²€ì¦ ì‹œì‘")
    
    try:
        job_id = get_job_id(**context)
        
        # íŒŒë¼ë¯¸í„° í™•ì¸
        skip_validation = context.get('params', {}).get('skip_validation', False)
        if skip_validation:
            logger.info("ë°ì´í„° ê²€ì¦ ìŠ¤í‚µ")
            raise AirflowSkipException("ë°ì´í„° ê²€ì¦ì´ ìŠ¤í‚µë˜ì—ˆìŠµë‹ˆë‹¤")
        
        content_result = context['task_instance'].xcom_pull(
            key='content_collection_result',
            task_ids='collect_law_contents'
        )
        
        if not content_result:
            raise AirflowException("ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ê¸°ë³¸ ê²€ì¦ ìˆ˜í–‰
        repository = LegalDataRepository()
        
        validation_result = {
            'job_id': job_id,
            'validation_passed': True,
            'total_laws_processed': content_result.get('processed_laws', 0),
            'total_errors': content_result.get('error_count', 0),
            'validation_time': datetime.now().isoformat()
        }
        
        # ì˜¤ë¥˜ê°€ ìˆëŠ” ê²½ìš° ê²€ì¦ ì‹¤íŒ¨ë¡œ ì²˜ë¦¬
        if content_result.get('error_count', 0) > 0:
            validation_result['validation_passed'] = False
            validation_result['validation_message'] = f"ë°ì´í„° ìˆ˜ì§‘ ì¤‘ {content_result['error_count']}ê±´ì˜ ì˜¤ë¥˜ ë°œìƒ"
        
        logger.info("ë°ì´í„° ê²€ì¦ ì™„ë£Œ", result=validation_result)
        
        # XComì— ì €ì¥
        context['task_instance'].xcom_push(key='validation_result', value=validation_result)
        
        return validation_result
        
    except AirflowSkipException:
        raise
    except Exception as e:
        logger.error("ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨", error=str(e))
        send_notification(
            title="ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨",
            message=f"ì‘ì—… ID: {get_job_id(**context)}\nì˜¤ë¥˜: {str(e)}",
            is_error=True,
            **context
        )
        raise

def update_sync_status(**context) -> Dict[str, Any]:
    """ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸"""
    logger.info("ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹œì‘")
    
    try:
        job_id = get_job_id(**context)
        
        validation_result = context['task_instance'].xcom_pull(
            key='validation_result',
            task_ids='validate_collected_data'
        )
        
        content_result = context['task_instance'].xcom_pull(
            key='content_collection_result',
            task_ids='collect_law_contents'
        )
        
        if not validation_result or not validation_result.get('validation_passed', False):
            logger.warning("ê²€ì¦ ì‹¤íŒ¨ë¡œ ì¸í•œ ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ ìŠ¤í‚µ")
            raise AirflowSkipException("ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨ë¡œ ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ë¥¼ ìŠ¤í‚µí•©ë‹ˆë‹¤")
        
        # ì²˜ë¦¬ëœ ë‚ ì§œ ì¤‘ ìµœì‹  ë‚ ì§œë¡œ ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸
        strategy_info = context['task_instance'].xcom_pull(
            key='update_strategy',
            task_ids='determine_update_strategy'
        )
        
        if strategy_info and strategy_info.get('dates_to_process'):
            latest_date = max([
                datetime.strptime(d, '%Y-%m-%d').date()
                for d in strategy_info['dates_to_process']
            ])
            
            # ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸
            repository = LegalDataRepository()
            success = repository.update_sync_status("INCREMENTAL", latest_date, 0, 0)
            
            result = {
                'job_id': job_id,
                'sync_status_updated': success,
                'last_processed_date': latest_date.isoformat(),
                'update_time': datetime.now().isoformat()
            }
            
            if success:
                logger.info("ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ", last_processed_date=latest_date)
            else:
                logger.error("ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
                raise AirflowException("ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")
        else:
            raise AirflowException("ì²˜ë¦¬ëœ ë‚ ì§œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return result
        
    except AirflowSkipException:
        raise
    except Exception as e:
        logger.error("ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨", error=str(e))
        send_notification(
            title="ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨",
            message=f"ì‘ì—… ID: {get_job_id(**context)}\nì˜¤ë¥˜: {str(e)}",
            is_error=True,
            **context
        )
        raise

def send_completion_notification(**context) -> None:
    """ì™„ë£Œ ì•Œë¦¼ ë°œì†¡"""
    logger.info("ì™„ë£Œ ì•Œë¦¼ ë°œì†¡ ì‹œì‘")
    
    try:
        job_id = get_job_id(**context)
        
        # ëª¨ë“  ê²°ê³¼ ìˆ˜ì§‘
        content_result = context['task_instance'].xcom_pull(
            key='content_collection_result',
            task_ids='collect_law_contents'
        )
        
        validation_result = context['task_instance'].xcom_pull(
            key='validation_result',
            task_ids='validate_collected_data'
        )
        
        sync_result = context['task_instance'].xcom_pull(
            key='sync_status_updated',
            task_ids='update_sync_status'
        )
        
        # í†µê³„ ì •ë³´ ìƒì„±
        if content_result and 'total_stats' in content_result:
            stats = content_result['total_stats']
            message = f"""
ë°°ì¹˜ ì‘ì—… ì™„ë£Œ - {job_id}

ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:
â€¢ ì‹ ê·œ ë²•ë ¹: {stats.get('total_new_laws', 0)}ê°œ
â€¢ ì—…ë°ì´íŠ¸ ë²•ë ¹: {stats.get('total_updated_laws', 0)}ê°œ  
â€¢ ì‹ ê·œ ì¡°í•­: {stats.get('total_new_articles', 0)}ê°œ
â€¢ ì—…ë°ì´íŠ¸ ì¡°í•­: {stats.get('total_updated_articles', 0)}ê°œ
â€¢ ì˜¤ë¥˜ ê±´ìˆ˜: {stats.get('total_errors', 0)}ê±´
â€¢ ì´ ì²˜ë¦¬ ì‹œê°„: {stats.get('total_processing_time', 0):.1f}ì´ˆ

âœ… ìƒíƒœ: {'ì„±ê³µ' if stats.get('total_errors', 0) == 0 else 'ë¶€ë¶„ ì„±ê³µ'}
            """.strip()
        else:
            processed_laws = content_result.get('processed_laws', 0) if content_result else 0
            error_count = content_result.get('error_count', 0) if content_result else 0
            
            message = f"""
ë°°ì¹˜ ì‘ì—… ì™„ë£Œ - {job_id}

ğŸ“Š ì²˜ë¦¬ ê²°ê³¼:
â€¢ ì²˜ë¦¬ëœ ë²•ë ¹: {processed_laws}ê°œ
â€¢ ì˜¤ë¥˜ ê±´ìˆ˜: {error_count}ê±´

âœ… ìƒíƒœ: {'ì„±ê³µ' if error_count == 0 else 'ë¶€ë¶„ ì„±ê³µ'}
            """.strip()
        
        # ì•Œë¦¼ ë°œì†¡
        is_error = (content_result and content_result.get('error_count', 0) > 0) or \
                  (validation_result and not validation_result.get('validation_passed', True))
        
        send_notification(
            title="ë²•ì œì²˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì™„ë£Œ",
            message=message,
            is_error=is_error,
            **context
        )
        
        logger.info("ì™„ë£Œ ì•Œë¦¼ ë°œì†¡ ì™„ë£Œ", job_id=job_id)
        
    except Exception as e:
        logger.error("ì™„ë£Œ ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨", error=str(e))
        # ì•Œë¦¼ ì‹¤íŒ¨ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨ë¡œ ì´ì–´ì§€ì§€ ì•Šë„ë¡ í•¨

def handle_failure(**context) -> None:
    """ì‹¤íŒ¨ ì²˜ë¦¬"""
    logger.error("íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨ ì²˜ë¦¬ ì‹œì‘")
    
    try:
        job_id = get_job_id(**context) if context.get('task_instance') else 'unknown'
        failed_task = context.get('task_instance', {}).task_id if context.get('task_instance') else 'unknown'
        
        # ì‹¤íŒ¨í•œ íƒœìŠ¤í¬ ì •ë³´ ìˆ˜ì§‘
        exception = context.get('exception')
        error_message = str(exception) if exception else "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"
        
        message = f"""
ğŸš¨ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨

ì‘ì—… ID: {job_id}
ì‹¤íŒ¨ íƒœìŠ¤í¬: {failed_task}
ì˜¤ë¥˜ ë©”ì‹œì§€: {error_message}
ì‹¤í–‰ ì‹œê°„: {context.get('execution_date', 'unknown')}

ê´€ë¦¬ì í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.
        """.strip()
        
        send_notification(
            title="ë²•ì œì²˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨",
            message=message,
            is_error=True,
            **context
        )
        
        logger.error("íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨ ì²˜ë¦¬ ì™„ë£Œ", 
                    job_id=job_id, 
                    failed_task=failed_task,
                    error=error_message)
        
    except Exception as e:
        logger.error("ì‹¤íŒ¨ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜", error=str(e))

# ==================== DAG íƒœìŠ¤í¬ ì •ì˜ ====================

# ì‘ì—… ID ìƒì„±
generate_job_id_task = PythonOperator(
    task_id='generate_job_id',
    python_callable=generate_job_id,
    dag=dag,
    doc_md="ë°°ì¹˜ ì‘ì—… ê³ ìœ  IDë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
)

# API ìƒíƒœ í™•ì¸
api_health_check_task = LegalAPIHealthCheckOperator(
    task_id='check_api_health',
    max_response_time=10000,
    fail_on_error=True,
    dag=dag,
    doc_md="ë²•ì œì²˜ API ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
)

# ì‚¬ì „ ì¡°ê±´ í™•ì¸
check_prerequisites_task = PythonOperator(
    task_id='check_prerequisites',
    python_callable=check_prerequisites,
    dag=dag,
    doc_md="ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë° ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
)

# ì—…ë°ì´íŠ¸ ì „ëµ ê²°ì •
determine_strategy_task = PythonOperator(
    task_id='determine_update_strategy',
    python_callable=determine_update_strategy,
    dag=dag,
    doc_md="ì¦ë¶„ ì—…ë°ì´íŠ¸ ì „ëµì„ ê²°ì •í•˜ê³  ì²˜ë¦¬í•  ë‚ ì§œë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
)

# ë°ì´í„° ìˆ˜ì§‘ (ì»¤ìŠ¤í…€ ì˜¤í¼ë ˆì´í„° ì‚¬ìš©)
collect_legal_data_task = LegalDataCollectionOperator(
    task_id='collect_legal_data',
    collection_type='incremental',
    batch_size=50,
    dag=dag,
    doc_md="ë²•ì œì²˜ APIì—ì„œ ì¦ë¶„ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤."
)

# ë°ì´í„° ê²€ì¦ (ì»¤ìŠ¤í…€ ì˜¤í¼ë ˆì´í„° ì‚¬ìš©)
validate_data_task = LegalDataValidationOperator(
    task_id='validate_collected_data',
    validation_rules={
        'max_error_rate': 0.05,  # 5% ì´í•˜ ì˜¤ë¥˜ìœ¨
        'max_processing_time': 3600,  # 1ì‹œê°„ ì´í•˜ ì²˜ë¦¬ ì‹œê°„
    },
    fail_on_validation_error=False,  # ê²€ì¦ ì‹¤íŒ¨ ì‹œì—ë„ íŒŒì´í”„ë¼ì¸ ê³„ì† ì§„í–‰
    dag=dag,
    doc_md="ìˆ˜ì§‘ëœ ë°ì´í„°ì˜ ì •í•©ì„±ì„ ê²€ì¦í•©ë‹ˆë‹¤."
)

# ë™ê¸°í™” ìƒíƒœ ì—…ë°ì´íŠ¸
update_sync_task = PythonOperator(
    task_id='update_sync_status',
    python_callable=update_sync_status,
    dag=dag,
    doc_md="ë§ˆì§€ë§‰ ë™ê¸°í™” ë‚ ì§œë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."
)

# ì™„ë£Œ ì•Œë¦¼ (ì»¤ìŠ¤í…€ ì˜¤í¼ë ˆì´í„° ì‚¬ìš©)
completion_notification_task = LegalDataNotificationOperator(
    task_id='send_completion_notification',
    notification_type='completion',
    notification_channels=['slack'],
    include_stats=True,
    dag=dag,
    trigger_rule='none_failed_or_skipped',
    doc_md="ë°°ì¹˜ ì‘ì—… ì™„ë£Œ ì•Œë¦¼ì„ ë°œì†¡í•©ë‹ˆë‹¤."
)

# ì‹¤íŒ¨ ì²˜ë¦¬ (ì»¤ìŠ¤í…€ ì˜¤í¼ë ˆì´í„° ì‚¬ìš©)
failure_handler_task = LegalDataNotificationOperator(
    task_id='handle_failure',
    notification_type='error',
    notification_channels=['slack', 'email'],
    dag=dag,
    trigger_rule='one_failed',
    doc_md="íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨ ì‹œ ì•Œë¦¼ì„ ë°œì†¡í•©ë‹ˆë‹¤."
)

# ì •ë¦¬ ì‘ì—… (ì»¤ìŠ¤í…€ ì˜¤í¼ë ˆì´í„° ì‚¬ìš©)
cleanup_task = LegalDataCleanupOperator(
    task_id='cleanup_temp_files',
    cleanup_days=7,
    cleanup_temp=True,
    cleanup_logs=True,
    dag=dag,
    trigger_rule='none_failed_or_skipped',
    doc_md="ì„ì‹œ íŒŒì¼ê³¼ ì˜¤ë˜ëœ ë¡œê·¸ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."
)

# ==================== íƒœìŠ¤í¬ ì˜ì¡´ì„± ì„¤ì • ====================

# ë©”ì¸ í”Œë¡œìš°
generate_job_id_task >> api_health_check_task >> check_prerequisites_task
check_prerequisites_task >> determine_strategy_task >> collect_legal_data_task
collect_legal_data_task >> validate_data_task >> update_sync_task
update_sync_task >> completion_notification_task >> cleanup_task

# ì‹¤íŒ¨ ì²˜ë¦¬ í”Œë¡œìš°
[api_health_check_task, check_prerequisites_task, determine_strategy_task, 
 collect_legal_data_task, validate_data_task, update_sync_task] >> failure_handler_task

# ==================== DAG ë ˆë²¨ ì½œë°± ====================

def dag_success_callback(context):
    """DAG ì„±ê³µ ì½œë°±"""
    logger.info("DAG ì‹¤í–‰ ì„±ê³µ", execution_date=context['execution_date'])

def dag_failure_callback(context):
    """DAG ì‹¤íŒ¨ ì½œë°±"""
    logger.error("DAG ì‹¤í–‰ ì‹¤íŒ¨", 
                execution_date=context['execution_date'],
                exception=context.get('exception'))
    
    # ê¸´ê¸‰ ì•Œë¦¼ ë°œì†¡
    try:
        slack_service.send_error_alert(
            title="ğŸš¨ ë²•ì œì²˜ íŒŒì´í”„ë¼ì¸ DAG ì‹¤íŒ¨",
            message=f"ì‹¤í–‰ ë‚ ì§œ: {context['execution_date']}\n"
                   f"ì˜¤ë¥˜: {context.get('exception', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}",
            context=context
        )
    except Exception as e:
        logger.error("DAG ì‹¤íŒ¨ ì½œë°± ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨", error=str(e))

# DAGì— ì½œë°± ì„¤ì •
dag.on_success_callback = dag_success_callback
dag.on_failure_callback = dag_failure_callback